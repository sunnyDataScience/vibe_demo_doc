# ğŸ“Š ç›£æ§èˆ‡ç¶­è­·æŒ‡å—

## ğŸ¯ ç›£æ§ç­–ç•¥æ¦‚è¿°

### ç›£æ§çš„å››å€‹é»ƒé‡‘ä¿¡è™Ÿ
```
å»¶é² (Latency)
â”œâ”€â”€ API éŸ¿æ‡‰æ™‚é–“
â”œâ”€â”€ è³‡æ–™åº«æŸ¥è©¢æ™‚é–“
â”œâ”€â”€ ç¬¬ä¸‰æ–¹æœå‹™èª¿ç”¨æ™‚é–“
â””â”€â”€ é é¢è¼‰å…¥æ™‚é–“

æµé‡ (Traffic)
â”œâ”€â”€ æ¯ç§’è«‹æ±‚æ•¸ (RPS)
â”œâ”€â”€ ä½µç™¼ç”¨æˆ¶æ•¸
â”œâ”€â”€ é é¢ç€è¦½é‡
â””â”€â”€ API èª¿ç”¨æ¬¡æ•¸

éŒ¯èª¤ (Errors)
â”œâ”€â”€ HTTP éŒ¯èª¤ç‡
â”œâ”€â”€ æ‡‰ç”¨ç¨‹å¼ç•°å¸¸
â”œâ”€â”€ è³‡æ–™åº«é€£æ¥å¤±æ•—
â””â”€â”€ ç¬¬ä¸‰æ–¹æœå‹™éŒ¯èª¤

é£½å’Œåº¦ (Saturation)
â”œâ”€â”€ CPU ä½¿ç”¨ç‡
â”œâ”€â”€ è¨˜æ†¶é«”ä½¿ç”¨ç‡
â”œâ”€â”€ ç£ç¢Ÿ I/O
â””â”€â”€ ç¶²è·¯é »å¯¬
```

### ç›£æ§å±¤ç´š
```
æ¥­å‹™ç›£æ§
â”œâ”€â”€ ç”¨æˆ¶è¨»å†Š/ç™»å…¥æˆåŠŸç‡
â”œâ”€â”€ è¨‚å–®è½‰æ›ç‡
â”œâ”€â”€ æ ¸å¿ƒåŠŸèƒ½ä½¿ç”¨ç‡
â””â”€â”€ æ”¶å…¥æŒ‡æ¨™

æ‡‰ç”¨ç¨‹å¼ç›£æ§
â”œâ”€â”€ API ç«¯é»æ€§èƒ½
â”œâ”€â”€ éŒ¯èª¤æ—¥èªŒåˆ†æ
â”œâ”€â”€ ä½¿ç”¨è€…é«”é©—æŒ‡æ¨™
â””â”€â”€ æ‡‰ç”¨ç¨‹å¼å¥åº·ç‹€æ…‹

åŸºç¤è¨­æ–½ç›£æ§
â”œâ”€â”€ ä¼ºæœå™¨è³‡æºä½¿ç”¨
â”œâ”€â”€ è³‡æ–™åº«æ€§èƒ½
â”œâ”€â”€ ç¶²è·¯é€£é€šæ€§
â””â”€â”€ å®‰å…¨äº‹ä»¶ç›£æ§
```

## ğŸ”§ ç›£æ§å·¥å…·è¨­ç½®

### 1. ç³»çµ±ç›£æ§ - Prometheus + Grafana

#### Prometheus é…ç½®
```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['localhost:9100']

  - job_name: 'nginx-exporter'
    static_configs:
      - targets: ['localhost:9113']

  - job_name: 'postgres-exporter'
    static_configs:
      - targets: ['localhost:9187']

  - job_name: 'app-metrics'
    static_configs:
      - targets: ['localhost:3000']
    metrics_path: /metrics

alerting:
  alertmanagers:
    - static_configs:
        - targets: ['localhost:9093']
```

#### Docker Compose ç›£æ§è¨­ç½®
```yaml
# docker-compose.monitoring.yml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3001:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/datasources:/etc/grafana/provisioning/datasources
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin123

  node-exporter:
    image: prom/node-exporter:latest
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'

  alertmanager:
    image: prom/alertmanager:latest
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml

volumes:
  prometheus_data:
  grafana_data:
```

### 2. æ‡‰ç”¨ç¨‹å¼ç›£æ§æ•´åˆ

#### Node.js æ‡‰ç”¨ç¨‹å¼ç›£æ§
```javascript
// monitoring.js
const promClient = require('prom-client');
const express = require('express');

// å»ºç«‹åº¦é‡æ”¶é›†å™¨
const collectDefaultMetrics = promClient.collectDefaultMetrics;
collectDefaultMetrics();

// è‡ªå®šç¾©åº¦é‡
const httpDuration = new promClient.Histogram({
  name: 'http_request_duration_ms',
  help: 'Duration of HTTP requests in ms',
  labelNames: ['route', 'status_code'],
  buckets: [0.1, 5, 15, 50, 100, 500]
});

const httpRequestsTotal = new promClient.Counter({
  name: 'http_requests_total',
  help: 'Total number of HTTP requests',
  labelNames: ['method', 'route', 'status_code']
});

const activeConnections = new promClient.Gauge({
  name: 'active_connections',
  help: 'Number of active connections'
});

const databaseConnections = new promClient.Gauge({
  name: 'database_connections_active',
  help: 'Number of active database connections'
});

// ä¸­ä»‹è»Ÿé«”
const metricsMiddleware = (req, res, next) => {
  const start = Date.now();

  res.on('finish', () => {
    const duration = Date.now() - start;
    const route = req.route ? req.route.path : req.path;

    httpDuration
      .labels(route, res.statusCode)
      .observe(duration);

    httpRequestsTotal
      .labels(req.method, route, res.statusCode)
      .inc();
  });

  next();
};

// å¥åº·æª¢æŸ¥ç«¯é»
app.get('/health', async (req, res) => {
  const healthCheck = {
    uptime: process.uptime(),
    message: 'OK',
    timestamp: Date.now(),
    checks: {
      database: await checkDatabase(),
      redis: await checkRedis(),
      externalApi: await checkExternalServices()
    }
  };

  try {
    res.status(200).json(healthCheck);
  } catch (error) {
    healthCheck.message = error.message;
    res.status(503).json(healthCheck);
  }
});

// åº¦é‡ç«¯é»
app.get('/metrics', async (req, res) => {
  // æ›´æ–°è³‡æ–™åº«é€£æ¥æ•¸
  databaseConnections.set(await getCurrentDbConnections());

  res.set('Content-Type', promClient.register.contentType);
  res.end(await promClient.register.metrics());
});

async function checkDatabase() {
  try {
    await db.raw('SELECT 1');
    return { status: 'healthy', responseTime: 0 };
  } catch (error) {
    return { status: 'unhealthy', error: error.message };
  }
}
```

### 3. æ—¥èªŒç®¡ç† - ELK Stack

#### Logstash é…ç½®
```ruby
# logstash.conf
input {
  file {
    path => "/var/log/nginx/access.log"
    start_position => "beginning"
    type => "nginx-access"
  }

  file {
    path => "/home/deploy/app/logs/*.log"
    start_position => "beginning"
    type => "app-logs"
    codec => "json"
  }
}

filter {
  if [type] == "nginx-access" {
    grok {
      match => {
        "message" => "%{NGINXACCESS}"
      }
    }

    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }

    mutate {
      convert => {
        "response" => "integer"
        "bytes" => "integer"
        "responsetime" => "float"
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }

  stdout { codec => rubydebug }
}
```

#### æ‡‰ç”¨ç¨‹å¼æ—¥èªŒçµæ§‹åŒ–
```javascript
// logger.js
const winston = require('winston');

const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.errors({ stack: true }),
    winston.format.json()
  ),
  defaultMeta: {
    service: 'myapp-api',
    version: process.env.npm_package_version
  },
  transports: [
    new winston.transports.File({
      filename: 'logs/error.log',
      level: 'error'
    }),
    new winston.transports.File({
      filename: 'logs/combined.log'
    })
  ]
});

// ç”Ÿç”¢ç’°å¢ƒæ·»åŠ  console è¼¸å‡º
if (process.env.NODE_ENV !== 'production') {
  logger.add(new winston.transports.Console({
    format: winston.format.simple()
  }));
}

// ä½¿ç”¨æ–¹å¼
logger.info('User login attempt', {
  userId: user.id,
  ip: req.ip,
  userAgent: req.get('User-Agent')
});

logger.error('Database connection failed', {
  error: error.message,
  stack: error.stack,
  query: failedQuery
});
```

## ğŸ“ˆ æ€§èƒ½ç›£æ§

### 1. æ‡‰ç”¨ç¨‹å¼æ€§èƒ½ç›£æ§ (APM)

#### New Relic æ•´åˆ
```javascript
// newrelic.js
'use strict';

exports.config = {
  app_name: ['MyApp API'],
  license_key: process.env.NEW_RELIC_LICENSE_KEY,
  logging: {
    level: 'info'
  },
  allow_all_headers: true,
  attributes: {
    exclude: [
      'request.headers.cookie',
      'request.headers.authorization',
      'request.headers.proxyAuthorization',
      'request.headers.setCookie*',
      'request.headers.x*',
      'response.headers.cookie',
      'response.headers.authorization',
      'response.headers.proxyAuthorization',
      'response.headers.setCookie*'
    ]
  }
};

// åœ¨æ‡‰ç”¨ç¨‹å¼æœ€å‰é¢å¼•å…¥
require('newrelic');
```

#### è‡ªå»º APM æŒ‡æ¨™
```javascript
// performance-monitoring.js
class PerformanceMonitor {
  constructor() {
    this.metrics = new Map();
  }

  trackApiCall(route, method) {
    return async (req, res, next) => {
      const start = process.hrtime.bigint();
      const key = `${method}:${route}`;

      res.on('finish', () => {
        const duration = Number(process.hrtime.bigint() - start) / 1000000; // ms

        this.recordMetric(key, {
          duration,
          statusCode: res.statusCode,
          timestamp: Date.now()
        });
      });

      next();
    };
  }

  recordMetric(key, data) {
    if (!this.metrics.has(key)) {
      this.metrics.set(key, []);
    }

    const metrics = this.metrics.get(key);
    metrics.push(data);

    // åªä¿ç•™æœ€è¿‘ 1000 ç­†è¨˜éŒ„
    if (metrics.length > 1000) {
      metrics.shift();
    }
  }

  getMetrics(key) {
    const metrics = this.metrics.get(key) || [];

    if (metrics.length === 0) return null;

    const durations = metrics.map(m => m.duration);
    const errors = metrics.filter(m => m.statusCode >= 400);

    return {
      count: metrics.length,
      avgDuration: durations.reduce((a, b) => a + b, 0) / durations.length,
      p95: this.percentile(durations, 0.95),
      p99: this.percentile(durations, 0.99),
      errorRate: errors.length / metrics.length,
      lastError: errors[errors.length - 1]
    };
  }

  percentile(arr, p) {
    const sorted = arr.sort((a, b) => a - b);
    const index = Math.ceil(sorted.length * p) - 1;
    return sorted[index];
  }
}

const monitor = new PerformanceMonitor();

// ä½¿ç”¨æ–¹å¼
app.get('/api/users',
  monitor.trackApiCall('/api/users', 'GET'),
  async (req, res) => {
    // API é‚è¼¯
  }
);
```

### 2. è³‡æ–™åº«æ€§èƒ½ç›£æ§

#### PostgreSQL ç›£æ§æŸ¥è©¢
```sql
-- æ…¢æŸ¥è©¢åˆ†æ
SELECT query,
       calls,
       total_time,
       mean_time,
       rows
FROM pg_stat_statements
ORDER BY total_time DESC
LIMIT 10;

-- è³‡æ–™åº«é€£æ¥æ•¸
SELECT count(*) AS connections,
       state
FROM pg_stat_activity
GROUP BY state;

-- è³‡æ–™åº«å¤§å°
SELECT pg_database.datname,
       pg_database_size(pg_database.datname) AS size
FROM pg_database
ORDER BY size DESC;

-- è¡¨æ ¼å¤§å°å’Œç´¢å¼•ä½¿ç”¨æƒ…æ³
SELECT schemaname,
       tablename,
       attname,
       n_distinct,
       correlation
FROM pg_stats
WHERE tablename = 'your_table_name';
```

#### Redis ç›£æ§
```javascript
// redis-monitor.js
const Redis = require('redis');

class RedisMonitor {
  constructor(redisClient) {
    this.client = redisClient;
  }

  async getInfo() {
    const info = await this.client.info();
    const lines = info.split('\r\n');
    const stats = {};

    lines.forEach(line => {
      if (line.includes(':')) {
        const [key, value] = line.split(':');
        stats[key] = isNaN(value) ? value : Number(value);
      }
    });

    return {
      memory: {
        used: stats.used_memory_human,
        peak: stats.used_memory_peak_human,
        fragmentation_ratio: stats.mem_fragmentation_ratio
      },
      stats: {
        connections: stats.connected_clients,
        commands_processed: stats.total_commands_processed,
        keyspace_hits: stats.keyspace_hits,
        keyspace_misses: stats.keyspace_misses
      },
      persistence: {
        rdb_last_save_time: stats.rdb_last_save_time,
        aof_enabled: stats.aof_enabled
      }
    };
  }

  async getSlowLog() {
    return await this.client.slowlog('get', 10);
  }
}
```

## ğŸš¨ å‘Šè­¦ç³»çµ±

### 1. Alertmanager é…ç½®
```yaml
# alertmanager.yml
global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@yourcompany.com'
  smtp_auth_username: 'alerts@yourcompany.com'
  smtp_auth_password: 'your-app-password'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'
  routes:
  - match:
      severity: critical
    receiver: 'critical-alerts'

receivers:
- name: 'web.hook'
  webhook_configs:
  - url: 'http://localhost:3000/webhook/alerts'

- name: 'critical-alerts'
  email_configs:
  - to: 'admin@yourcompany.com'
    subject: 'Critical Alert: {{ .GroupLabels.alertname }}'
    body: |
      {{ range .Alerts }}
      Alert: {{ .Annotations.summary }}
      Description: {{ .Annotations.description }}
      {{ end }}

  slack_configs:
  - api_url: 'your-slack-webhook-url'
    channel: '#alerts'
    title: 'Critical Alert'
    text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
```

### 2. å‘Šè­¦è¦å‰‡å®šç¾©
```yaml
# alert_rules.yml
groups:
- name: application
  rules:
  - alert: HighErrorRate
    expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.1
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "High error rate detected"
      description: "Error rate is {{ $value | humanizePercentage }}"

  - alert: HighResponseTime
    expr: histogram_quantile(0.95, rate(http_request_duration_ms_bucket[5m])) > 500
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High response time detected"
      description: "95th percentile response time is {{ $value }}ms"

  - alert: HighCPUUsage
    expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High CPU usage"
      description: "CPU usage is {{ $value }}%"

  - alert: HighMemoryUsage
    expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.9
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "High memory usage"
      description: "Memory usage is {{ $value | humanizePercentage }}"

  - alert: DiskSpaceLow
    expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) < 0.1
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Low disk space"
      description: "Disk space is {{ $value | humanizePercentage }} full"

  - alert: DatabaseDown
    expr: up{job="postgres-exporter"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Database is down"
      description: "PostgreSQL database is not responding"
```

### 3. Webhook å‘Šè­¦è™•ç†
```javascript
// webhook-handler.js
app.post('/webhook/alerts', express.json(), (req, res) => {
  const alerts = req.body.alerts;

  alerts.forEach(alert => {
    const severity = alert.labels.severity;
    const alertname = alert.labels.alertname;
    const description = alert.annotations.description;

    logger.warn('Alert received', {
      alertname,
      severity,
      description,
      status: alert.status
    });

    // æ ¹æ“šåš´é‡æ€§åŸ·è¡Œä¸åŒæ“ä½œ
    switch (severity) {
      case 'critical':
        handleCriticalAlert(alert);
        break;
      case 'warning':
        handleWarningAlert(alert);
        break;
      default:
        handleInfoAlert(alert);
    }
  });

  res.status(200).send('OK');
});

async function handleCriticalAlert(alert) {
  // ç™¼é€ç·Šæ€¥é€šçŸ¥
  await sendSlackNotification(alert);
  await sendSMSAlert(alert);

  // è‡ªå‹•åŸ·è¡Œç·Šæ€¥ä¿®å¾©æªæ–½
  if (alert.labels.alertname === 'DatabaseDown') {
    await restartDatabaseService();
  }
}

async function sendSlackNotification(alert) {
  const webhook = new IncomingWebhook(process.env.SLACK_WEBHOOK_URL);

  await webhook.send({
    text: `ğŸš¨ Critical Alert: ${alert.labels.alertname}`,
    attachments: [{
      color: 'danger',
      fields: [{
        title: 'Description',
        value: alert.annotations.description,
        short: false
      }, {
        title: 'Severity',
        value: alert.labels.severity,
        short: true
      }]
    }]
  });
}
```

## ğŸ” æ—¥èªŒåˆ†æèˆ‡é™¤éŒ¯

### 1. çµæ§‹åŒ–æ—¥èªŒæœå°‹
```javascript
// æ—¥èªŒæœå°‹ API
app.get('/api/logs/search', async (req, res) => {
  const {
    query,
    level,
    service,
    startTime,
    endTime,
    limit = 100
  } = req.query;

  try {
    const searchParams = {
      index: 'logs-*',
      body: {
        query: {
          bool: {
            must: [
              query ? { match: { message: query } } : { match_all: {} }
            ],
            filter: [
              level ? { term: { level } } : null,
              service ? { term: { service } } : null,
              startTime && endTime ? {
                range: {
                  '@timestamp': {
                    gte: startTime,
                    lte: endTime
                  }
                }
              } : null
            ].filter(Boolean)
          }
        },
        sort: [{ '@timestamp': { order: 'desc' } }],
        size: limit
      }
    };

    const result = await elasticsearchClient.search(searchParams);
    res.json(result.body.hits.hits.map(hit => hit._source));
  } catch (error) {
    logger.error('Log search failed', { error: error.message });
    res.status(500).json({ error: 'Search failed' });
  }
});
```

### 2. éŒ¯èª¤è¿½è¹¤èˆ‡åˆ†æ
```javascript
// éŒ¯èª¤èšåˆåˆ†æ
class ErrorAnalyzer {
  constructor() {
    this.errorCounts = new Map();
  }

  trackError(error, context = {}) {
    const errorKey = this.generateErrorKey(error);

    if (!this.errorCounts.has(errorKey)) {
      this.errorCounts.set(errorKey, {
        count: 0,
        firstSeen: Date.now(),
        lastSeen: Date.now(),
        message: error.message,
        stack: error.stack,
        contexts: []
      });
    }

    const errorData = this.errorCounts.get(errorKey);
    errorData.count++;
    errorData.lastSeen = Date.now();
    errorData.contexts.push({
      timestamp: Date.now(),
      ...context
    });

    // åªä¿ç•™æœ€è¿‘ 10 å€‹ context
    if (errorData.contexts.length > 10) {
      errorData.contexts.shift();
    }

    // è¨˜éŒ„åˆ°æ—¥èªŒ
    logger.error('Error tracked', {
      errorKey,
      count: errorData.count,
      message: error.message,
      stack: error.stack,
      context
    });
  }

  generateErrorKey(error) {
    // ä½¿ç”¨éŒ¯èª¤è¨Šæ¯å’Œå †ç–Šè¿½è¹¤çš„å‰å¹¾è¡Œç”Ÿæˆå”¯ä¸€éµ
    const stackLines = error.stack.split('\n').slice(0, 3).join('\n');
    return crypto.createHash('md5')
      .update(error.message + stackLines)
      .digest('hex');
  }

  getTopErrors(limit = 10) {
    return Array.from(this.errorCounts.entries())
      .sort(([,a], [,b]) => b.count - a.count)
      .slice(0, limit)
      .map(([key, data]) => ({ key, ...data }));
  }
}

const errorAnalyzer = new ErrorAnalyzer();

// å…¨å±€éŒ¯èª¤è™•ç†
process.on('uncaughtException', (error) => {
  errorAnalyzer.trackError(error, { type: 'uncaughtException' });
  logger.error('Uncaught Exception', { error: error.message, stack: error.stack });
  process.exit(1);
});

process.on('unhandledRejection', (reason, promise) => {
  errorAnalyzer.trackError(new Error(reason), {
    type: 'unhandledRejection',
    promise: promise.toString()
  });
  logger.error('Unhandled Rejection', { reason, promise });
});
```

## ğŸ“Š å•†æ¥­æŒ‡æ¨™ç›£æ§

### 1. ç”¨æˆ¶è¡Œç‚ºåˆ†æ
```javascript
// ç”¨æˆ¶äº‹ä»¶è¿½è¹¤
class UserAnalytics {
  constructor() {
    this.events = new Map();
  }

  trackEvent(userId, eventType, properties = {}) {
    const event = {
      userId,
      eventType,
      properties,
      timestamp: Date.now(),
      sessionId: this.getSessionId(userId)
    };

    // ç™¼é€åˆ°åˆ†æå¹³å°
    this.sendToAnalytics(event);

    // æ›´æ–°å³æ™‚æŒ‡æ¨™
    this.updateRealTimeMetrics(event);
  }

  updateRealTimeMetrics(event) {
    const today = new Date().toISOString().split('T')[0];
    const key = `${today}:${event.eventType}`;

    if (!this.events.has(key)) {
      this.events.set(key, 0);
    }

    this.events.set(key, this.events.get(key) + 1);
  }

  getDailyMetrics(date) {
    const prefix = `${date}:`;
    const metrics = {};

    for (const [key, value] of this.events.entries()) {
      if (key.startsWith(prefix)) {
        const eventType = key.replace(prefix, '');
        metrics[eventType] = value;
      }
    }

    return metrics;
  }
}

const analytics = new UserAnalytics();

// è¿½è¹¤ç”¨æˆ¶ç™»å…¥
app.post('/api/auth/login', async (req, res) => {
  try {
    const user = await authenticateUser(req.body);

    analytics.trackEvent(user.id, 'login', {
      loginMethod: req.body.method,
      ip: req.ip,
      userAgent: req.get('User-Agent')
    });

    res.json({ success: true, user });
  } catch (error) {
    analytics.trackEvent(null, 'login_failed', {
      reason: error.message,
      ip: req.ip
    });

    res.status(401).json({ error: 'Login failed' });
  }
});

// è¿½è¹¤åŠŸèƒ½ä½¿ç”¨
app.post('/api/features/:feature/use', auth, async (req, res) => {
  analytics.trackEvent(req.user.id, 'feature_use', {
    feature: req.params.feature,
    parameters: req.body
  });

  // è™•ç†åŠŸèƒ½é‚è¼¯...
});
```

### 2. å•†æ¥­æŒ‡æ¨™å„€è¡¨æ¿
```javascript
// å•†æ¥­æŒ‡æ¨™ API
app.get('/api/metrics/business', async (req, res) => {
  const { startDate, endDate } = req.query;

  try {
    const metrics = await Promise.all([
      getUserMetrics(startDate, endDate),
      getRevenueMetrics(startDate, endDate),
      getUsageMetrics(startDate, endDate),
      getConversionMetrics(startDate, endDate)
    ]);

    res.json({
      users: metrics[0],
      revenue: metrics[1],
      usage: metrics[2],
      conversion: metrics[3],
      timestamp: Date.now()
    });
  } catch (error) {
    logger.error('Business metrics fetch failed', { error: error.message });
    res.status(500).json({ error: 'Metrics unavailable' });
  }
});

async function getUserMetrics(startDate, endDate) {
  return {
    totalUsers: await User.count(),
    newUsers: await User.count({
      where: {
        createdAt: {
          [Op.between]: [startDate, endDate]
        }
      }
    }),
    activeUsers: await getActiveUsers(startDate, endDate),
    retentionRate: await calculateRetentionRate()
  };
}

async function getUsageMetrics(startDate, endDate) {
  const events = await AnalyticsEvent.findAll({
    where: {
      timestamp: {
        [Op.between]: [startDate, endDate]
      }
    },
    attributes: [
      'eventType',
      [Sequelize.fn('COUNT', Sequelize.col('id')), 'count']
    ],
    group: ['eventType']
  });

  return events.reduce((acc, event) => {
    acc[event.eventType] = parseInt(event.getDataValue('count'));
    return acc;
  }, {});
}
```

## ğŸ”§ ç¶­è­·ä»»å‹™è‡ªå‹•åŒ–

### 1. å®šæœŸæ¸…ç†è…³æœ¬
```bash
#!/bin/bash
# cleanup.sh - å®šæœŸç¶­è­·è…³æœ¬

LOG_FILE="/var/log/maintenance.log"
DATE=$(date '+%Y-%m-%d %H:%M:%S')

echo "[$DATE] Starting maintenance tasks..." >> $LOG_FILE

# æ¸…ç†èˆŠæ—¥èªŒæ–‡ä»¶ (ä¿ç•™ 30 å¤©)
find /home/deploy/app/logs -name "*.log" -mtime +30 -delete
echo "[$DATE] Cleaned old log files" >> $LOG_FILE

# æ¸…ç†è³‡æ–™åº«å‚™ä»½ (ä¿ç•™ 7 å¤©)
find /home/deploy/backups -name "db_*.sql" -mtime +7 -delete
echo "[$DATE] Cleaned old database backups" >> $LOG_FILE

# æ¸…ç† Docker æ˜ åƒå’Œå®¹å™¨
docker system prune -f
echo "[$DATE] Cleaned Docker system" >> $LOG_FILE

# æ›´æ–°ç³»çµ±å¥—ä»¶
apt update && apt upgrade -y
echo "[$DATE] Updated system packages" >> $LOG_FILE

# æª¢æŸ¥ç£ç¢Ÿç©ºé–“
DISK_USAGE=$(df -h / | awk 'NR==2{print $5}' | sed 's/%//')
if [ $DISK_USAGE -gt 80 ]; then
    echo "[$DATE] WARNING: Disk usage is ${DISK_USAGE}%" >> $LOG_FILE
    # ç™¼é€å‘Šè­¦
    curl -X POST $WEBHOOK_URL -d "Disk usage is ${DISK_USAGE}%"
fi

# é‡å•Ÿæœå‹™ (æ¯é€±)
if [ $(date +%u) -eq 7 ]; then
    pm2 restart all
    systemctl restart nginx
    echo "[$DATE] Restarted services (weekly)" >> $LOG_FILE
fi

echo "[$DATE] Maintenance tasks completed" >> $LOG_FILE
```

### 2. å‚™ä»½ç­–ç•¥
```bash
#!/bin/bash
# backup.sh - å®Œæ•´å‚™ä»½è…³æœ¬

BACKUP_DIR="/home/deploy/backups"
DATE=$(date +%Y%m%d_%H%M%S)
APP_DIR="/home/deploy/your-project"

# å»ºç«‹å‚™ä»½ç›®éŒ„
mkdir -p $BACKUP_DIR/database
mkdir -p $BACKUP_DIR/uploads
mkdir -p $BACKUP_DIR/config

# è³‡æ–™åº«å‚™ä»½
echo "Backing up database..."
pg_dump myapp_production | gzip > $BACKUP_DIR/database/db_$DATE.sql.gz

# ä¸Šå‚³æ–‡ä»¶å‚™ä»½
echo "Backing up uploads..."
tar -czf $BACKUP_DIR/uploads/uploads_$DATE.tar.gz $APP_DIR/uploads/

# é…ç½®æ–‡ä»¶å‚™ä»½
echo "Backing up configuration..."
cp $APP_DIR/.env $BACKUP_DIR/config/env_$DATE
cp /etc/nginx/sites-available/myapp $BACKUP_DIR/config/nginx_$DATE

# ä¸Šå‚³åˆ°é›²ç«¯å„²å­˜ (AWS S3 ç¯„ä¾‹)
if command -v aws &> /dev/null; then
    echo "Uploading to S3..."
    aws s3 sync $BACKUP_DIR s3://your-backup-bucket/$(date +%Y/%m/%d)/
fi

# æ¸…ç†èˆŠå‚™ä»½ (ä¿ç•™ 30 å¤©)
find $BACKUP_DIR -type f -mtime +30 -delete

echo "Backup completed: $DATE"
```

### 3. å¥åº·æª¢æŸ¥è‡ªå‹•ä¿®å¾©
```bash
#!/bin/bash
# health-check.sh - å¥åº·æª¢æŸ¥å’Œè‡ªå‹•ä¿®å¾©

check_service() {
    local service=$1
    local url=$2

    if curl -f -s $url > /dev/null; then
        echo "âœ… $service is healthy"
        return 0
    else
        echo "âŒ $service is unhealthy"
        return 1
    fi
}

restart_service() {
    local service=$1
    echo "ğŸ”„ Restarting $service..."

    case $service in
        "app")
            pm2 restart all
            ;;
        "nginx")
            systemctl restart nginx
            ;;
        "postgresql")
            systemctl restart postgresql
            ;;
        *)
            echo "Unknown service: $service"
            ;;
    esac
}

# æª¢æŸ¥æ‡‰ç”¨ç¨‹å¼
if ! check_service "Application" "http://localhost:3000/health"; then
    restart_service "app"
    sleep 10

    if ! check_service "Application" "http://localhost:3000/health"; then
        echo "ğŸš¨ Application still unhealthy after restart"
        curl -X POST $ALERT_WEBHOOK -d "Application health check failed after restart"
    fi
fi

# æª¢æŸ¥ Nginx
if ! check_service "Nginx" "http://localhost/health"; then
    restart_service "nginx"
    sleep 5

    if ! check_service "Nginx" "http://localhost/health"; then
        echo "ğŸš¨ Nginx still unhealthy after restart"
    fi
fi

# æª¢æŸ¥è³‡æ–™åº«é€£æ¥
if ! pg_isready -h localhost -p 5432; then
    echo "âŒ PostgreSQL is not ready"
    restart_service "postgresql"
    sleep 15

    if ! pg_isready -h localhost -p 5432; then
        echo "ğŸš¨ PostgreSQL still unhealthy after restart"
        curl -X POST $ALERT_WEBHOOK -d "PostgreSQL health check failed"
    fi
fi

# æª¢æŸ¥ç£ç¢Ÿç©ºé–“
DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
if [ $DISK_USAGE -gt 85 ]; then
    echo "ğŸš¨ Disk usage is ${DISK_USAGE}%"
    # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
    rm -rf /tmp/*
    # æ¸…ç† Docker
    docker system prune -f
fi

# æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨
MEMORY_USAGE=$(free | grep Mem | awk '{printf("%.2f", $3/$2 * 100.0)}')
if [ $(echo "$MEMORY_USAGE > 90" | bc) -eq 1 ]; then
    echo "ğŸš¨ Memory usage is ${MEMORY_USAGE}%"
    # é‡å•Ÿæ‡‰ç”¨ç¨‹å¼é‡‹æ”¾è¨˜æ†¶é«”
    pm2 restart all
fi
```

## ğŸ“‹ ç¶­è­·æª¢æŸ¥æ¸…å–®

### æ—¥å¸¸æª¢æŸ¥ (æ¯å¤©)
```markdown
- [ ] æª¢æŸ¥æ‡‰ç”¨ç¨‹å¼æ—¥èªŒéŒ¯èª¤
- [ ] ç¢ºèªç›£æ§æŒ‡æ¨™æ­£å¸¸
- [ ] æª¢æŸ¥å‚™ä»½æ˜¯å¦æˆåŠŸ
- [ ] ç¢ºèªç£ç¢Ÿç©ºé–“å……è¶³
- [ ] æŸ¥çœ‹å®‰å…¨äº‹ä»¶æ—¥èªŒ
- [ ] æª¢æŸ¥ç”¨æˆ¶åé¥‹å’Œå•é¡Œ
```

### é€±åº¦æª¢æŸ¥ (æ¯é€±)
```markdown
- [ ] æª¢æŸ¥ç³»çµ±æ›´æ–°
- [ ] åˆ†ææ€§èƒ½è¶¨å‹¢
- [ ] æª¢æŸ¥ SSL æ†‘è­‰éæœŸæ™‚é–“
- [ ] æ¸…ç†èˆŠæ—¥èªŒå’Œå‚™ä»½
- [ ] æ›´æ–°æ–‡ä»¶å’Œ runbook
- [ ] æª¢æŸ¥ä¾è³´å¥—ä»¶å®‰å…¨æ€§æ›´æ–°
```

### æœˆåº¦æª¢æŸ¥ (æ¯æœˆ)
```markdown
- [ ] å…¨é¢å®‰å…¨æ€§æƒæ
- [ ] æª¢æŸ¥è³‡æ–™åº«æ€§èƒ½å’Œå„ªåŒ–
- [ ] åˆ†ææˆæœ¬å’Œä½¿ç”¨é‡
- [ ] æ›´æ–°ç½é›£æ¢å¾©è¨ˆåŠƒ
- [ ] é€²è¡Œæ•…éšœæ¼”ç·´
- [ ] æª¢æŸ¥åˆè¦æ€§è¦æ±‚
```

## ğŸ“š å·¥å…·å’Œè³‡æº

### æ¨è–¦ç›£æ§å·¥å…·
```markdown
å…è²»/é–‹æº:
â”œâ”€â”€ Prometheus + Grafana (æŒ‡æ¨™ç›£æ§)
â”œâ”€â”€ ELK Stack (æ—¥èªŒåˆ†æ)
â”œâ”€â”€ Zabbix (åŸºç¤è¨­æ–½ç›£æ§)
â”œâ”€â”€ Nagios (å‚³çµ±ç›£æ§)
â””â”€â”€ Netdata (å³æ™‚ç›£æ§)

å•†æ¥­è§£æ±ºæ–¹æ¡ˆ:
â”œâ”€â”€ New Relic (APM)
â”œâ”€â”€ DataDog (å…¨æ–¹ä½ç›£æ§)
â”œâ”€â”€ Splunk (æ—¥èªŒåˆ†æ)
â”œâ”€â”€ PagerDuty (å‘Šè­¦ç®¡ç†)
â””â”€â”€ Pingdom (å¤–éƒ¨ç›£æ§)
```

### å­¸ç¿’è³‡æº
- [SRE Book](https://sre.google/books/) - Google SRE å¯¦è¸
- [Monitoring 101](https://www.datadoghq.com/blog/monitoring-101-collecting-data/) - ç›£æ§åŸºç¤
- [The Art of Monitoring](https://artofmonitoring.com/) - ç›£æ§è—è¡“
- [Observability Engineering](https://www.oreilly.com/library/view/observability-engineering/9781492076438/) - å¯è§€æ¸¬æ€§å·¥ç¨‹

æœ¬æŒ‡å—æä¾›äº†å…¨é¢çš„ç›£æ§èˆ‡ç¶­è­·ç­–ç•¥ï¼Œå¹«åŠ©ç¢ºä¿ç³»çµ±ç©©å®šé‹è¡Œä¸¦å¿«é€Ÿç™¼ç¾å’Œè§£æ±ºå•é¡Œã€‚è¨˜ä½ï¼Œè‰¯å¥½çš„ç›£æ§æ˜¯é é˜²å•é¡Œè€Œä¸æ˜¯è¢«å‹•éŸ¿æ‡‰ï¼