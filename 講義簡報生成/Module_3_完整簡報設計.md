# Module 3ï¼šè³‡æ–™å“è³ªèˆ‡è™•ç† - å®Œæ•´ç°¡å ±è¨­è¨ˆ
## ğŸ¯ 45åˆ†é˜æŒæ¡è³‡æ–™æ¸…ç†è—è¡“

---

## ğŸ“Š ç°¡å ±åŸºæœ¬è³‡è¨Š
- **ç¸½é æ•¸**ï¼š20é 
- **é è¨ˆæ™‚é•·**ï¼š45åˆ†é˜
- **è¨­è¨ˆä¸»è‰²**ï¼šæˆåŠŸç¶  (#52B788)
- **è¼”åŠ©è‰²å½©**ï¼šè­¦å‘Šæ©™ (#F77F00)

---

## ğŸ¬ P1ï¼šç« ç¯€å°é¢ [å•é¡Œæ„è­˜]
### ç‰ˆé¢é…ç½®
```
èƒŒæ™¯ï¼šé«’æ•¸æ“š â†’ ä¹¾æ·¨æ•¸æ“šçš„æ¼¸è®Šæ•ˆæœ
å·¦å´ï¼šå¤§æ•¸å­— "3" (é€æ˜åº¦ 15%)
å³å´ï¼šå…§å®¹å€åŸŸ
```

### å…§å®¹æ–‡å­—
**ä¸»æ¨™é¡Œ** (40pt, ç¶ è‰²)
```
Module 3
è³‡æ–™å“è³ªèˆ‡è™•ç†
```

**å‰¯æ¨™é¡Œ** (24pt)
```
åƒåœ¾é€²ï¼Œåƒåœ¾å‡º (GIGO)
æŠŠé«’æ•¸æ“šè®Šæˆå¯¶è—
```

**æœ¬ç¯€é‡é»** (20pt, åœ–æ¨™åˆ—è¡¨)
```
ğŸ” å“è³ªæª¢æŸ¥ï¼šæ‰¾å‡ºè³‡æ–™å•é¡Œ
ğŸ§¹ è³‡æ–™æ¸…ç†ï¼šè™•ç†ç¼ºå¤±èˆ‡ç•°å¸¸
ğŸ”§ è³‡æ–™è½‰æ›ï¼šæ ¼å¼æ¨™æº–åŒ–
âœ¨ ç‰¹å¾µå·¥ç¨‹ï¼šå‰µé€ æ–°åƒ¹å€¼
ğŸ“Š å“è³ªå ±å‘Šï¼šç¢ºä¿å¯ä¿¡åº¦
```

### é€²åº¦æŒ‡ç¤º
```
[â– â– â– â–¡â–¡â–¡â–¡] Module 3/7
é è¨ˆæ™‚é–“ï¼š45åˆ†é˜
```

### å‹•ç•«æ•ˆæœ
- é«’æ•¸æ“šé€æ¼¸è®Šä¹¾æ·¨çš„å‹•ç•«
- é‡é»é …ç›®ä¾åºæµ®ç¾

### è¬›å¸«å£è¿°
```
"è³‡æ–™å“è³ªæ±ºå®šåˆ†æå“è³ªã€‚
åƒåœ¾é€²ï¼Œåƒåœ¾å‡º - é€™æ˜¯æ•¸æ“šåˆ†æçš„éµå¾‹ã€‚
é€™å ‚èª²æ•™ä½ æŠŠé«’æ•¸æ“šè®Šæˆé»ƒé‡‘ï¼"
```

---

## ğŸ¬ P2ï¼šç‚ºä»€éº¼è³‡æ–™å“è³ªé‡è¦ [å‹•æ©Ÿå»ºç«‹]
### ç‰ˆé¢é…ç½®
```
æ¡ˆä¾‹å±•ç¤º + æå¤±çµ±è¨ˆ
```

### å…§å®¹æ–‡å­—
**æ¨™é¡Œ** (32pt)
```
ğŸ’° é«’æ•¸æ“šçš„ä»£åƒ¹
```

**çœŸå¯¦æ¡ˆä¾‹**
```
ğŸ¥ é†«ç™‚æ¡ˆä¾‹ï¼š
éŒ¯èª¤è³‡æ–™ â†’ éŒ¯èª¤è¨ºæ–· â†’ ç”Ÿå‘½å±éšª

ğŸ¦ é‡‘èæ¡ˆä¾‹ï¼š
é‡è¤‡è¨˜éŒ„ â†’ å¤šæ¬¡æ‰£æ¬¾ â†’ å®¢æˆ¶æŠ•è¨´

ğŸ›’ é›¶å”®æ¡ˆä¾‹ï¼š
ç¼ºå¤±è³‡æ–™ â†’ åº«å­˜éŒ¯èª¤ â†’ ç¼ºè²¨æå¤±
```

**æ•¸æ“šå“è³ªæˆæœ¬**
```
IBM ç ”ç©¶ï¼š
â€¢ ç¾åœ‹ä¼æ¥­æ¯å¹´æå¤± $3.1 å…†ç¾å…ƒ
â€¢ 80% æ•¸æ“šç§‘å­¸å®¶æ™‚é–“èŠ±åœ¨æ¸…ç†
â€¢ é«’æ•¸æ“šå°è‡´ 40% è¨ˆç•«å¤±æ•—

å°ç£ç¾æ³ï¼š
â€¢ 60% ä¼æ¥­è³‡æ–™å“è³ªä¸ä½³
â€¢ å¹³å‡ 20% è³‡æ–™æœ‰ç¼ºå¤±
â€¢ æ¸…ç†æ™‚é–“ > åˆ†ææ™‚é–“
```

### å“è³ªç¶­åº¦
```
ğŸ“Š è³‡æ–™å“è³ªå…­å¤§ç¶­åº¦ï¼š
1. å®Œæ•´æ€§ (Completeness) - æœ‰ç¼ºå¤±å—ï¼Ÿ
2. æº–ç¢ºæ€§ (Accuracy) - æ­£ç¢ºå—ï¼Ÿ
3. ä¸€è‡´æ€§ (Consistency) - çŸ›ç›¾å—ï¼Ÿ
4. åŠæ™‚æ€§ (Timeliness) - å¤ æ–°å—ï¼Ÿ
5. å”¯ä¸€æ€§ (Uniqueness) - é‡è¤‡å—ï¼Ÿ
6. æœ‰æ•ˆæ€§ (Validity) - åˆç†å—ï¼Ÿ
```

### è¦–è¦ºå…ƒç´ 
- æå¤±é‡‘é¡æ»¾å‹•è¨ˆæ•¸
- å“è³ªç¶­åº¦é›·é”åœ–

### å‹•ç•«æ•ˆæœ
- æ¡ˆä¾‹é€ä¸€å±•ç¤º
- æ•¸å­—éœ‡æ’¼æ•ˆæœ

### è¬›å¸«å£è¿°
```
"æ¯å€‹éŒ¯èª¤è³‡æ–™éƒ½æ˜¯æ½›åœ¨çš„å•†æ¥­é¢¨éšªã€‚
æŠ•è³‡åœ¨è³‡æ–™å“è³ªï¼Œå ±é…¬ç‡è¶…é 10 å€ï¼"
```

---

## ğŸ¬ P3ï¼šè³‡æ–™å“è³ªæª¢æŸ¥æ¸…å–® [ç³»çµ±æ–¹æ³•]
### ç‰ˆé¢é…ç½®
```
æª¢æŸ¥æ¸…å–® + ç¨‹å¼ç¢¼å¯¦ä½œ
```

### å…§å®¹æ–‡å­—
**æ¨™é¡Œ** (32pt)
```
ğŸ“‹ ç³»çµ±åŒ–å“è³ªæª¢æŸ¥
```

**æª¢æŸ¥æ¸…å–®**
```python
# å®Œæ•´çš„è³‡æ–™å“è³ªæª¢æŸ¥æµç¨‹
import pandas as pd
import numpy as np

def data_quality_report(df):
    """ç”¢ç”Ÿè³‡æ–™å“è³ªå ±å‘Š"""

    print("="*50)
    print("ğŸ“Š è³‡æ–™å“è³ªå ±å‘Š")
    print("="*50)

    # 1. åŸºæœ¬è³‡è¨Š
    print(f"\n1ï¸âƒ£ åŸºæœ¬è³‡è¨Š")
    print(f"è³‡æ–™ç­†æ•¸: {len(df):,}")
    print(f"æ¬„ä½æ•¸é‡: {df.shape[1]}")
    print(f"è¨˜æ†¶é«”ä½¿ç”¨: {df.memory_usage().sum()/1024**2:.2f} MB")

    # 2. ç¼ºå¤±å€¼æª¢æŸ¥
    print(f"\n2ï¸âƒ£ ç¼ºå¤±å€¼æª¢æŸ¥")
    missing = df.isnull().sum()
    missing_pct = (missing / len(df) * 100).round(2)
    missing_df = pd.DataFrame({
        'ç¼ºå¤±æ•¸é‡': missing,
        'ç¼ºå¤±æ¯”ä¾‹': missing_pct
    })
    print(missing_df[missing_df['ç¼ºå¤±æ•¸é‡'] > 0])

    # 3. é‡è¤‡å€¼æª¢æŸ¥
    print(f"\n3ï¸âƒ£ é‡è¤‡å€¼æª¢æŸ¥")
    duplicates = df.duplicated().sum()
    print(f"é‡è¤‡åˆ—æ•¸: {duplicates} ({duplicates/len(df)*100:.1f}%)")

    # 4. è³‡æ–™å‹æ…‹æª¢æŸ¥
    print(f"\n4ï¸âƒ£ è³‡æ–™å‹æ…‹")
    print(df.dtypes.value_counts())

    # 5. ç•°å¸¸å€¼åˆæ­¥æª¢æŸ¥ï¼ˆæ•¸å€¼æ¬„ä½ï¼‰
    print(f"\n5ï¸âƒ£ ç•°å¸¸å€¼æª¢æŸ¥ï¼ˆæ•¸å€¼æ¬„ä½ï¼‰")
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        outliers = df[(df[col] < Q1 - 1.5*IQR) |
                     (df[col] > Q3 + 1.5*IQR)][col].count()
        if outliers > 0:
            print(f"{col}: {outliers} å€‹ç•°å¸¸å€¼")
```

### å¯¦éš›åŸ·è¡Œçµæœ
```
ğŸ“Š è³‡æ–™å“è³ªå ±å‘Š
==================================================

1ï¸âƒ£ åŸºæœ¬è³‡è¨Š
è³‡æ–™ç­†æ•¸: 1,000
æ¬„ä½æ•¸é‡: 29
è¨˜æ†¶é«”ä½¿ç”¨: 0.22 MB

2ï¸âƒ£ ç¼ºå¤±å€¼æª¢æŸ¥
        ç¼ºå¤±æ•¸é‡  ç¼ºå¤±æ¯”ä¾‹
Email      234    23.4%
Phone       56     5.6%

3ï¸âƒ£ é‡è¤‡å€¼æª¢æŸ¥
é‡è¤‡åˆ—æ•¸: 12 (1.2%)

4ï¸âƒ£ è³‡æ–™å‹æ…‹
object     15
float64     8
int64       4
datetime    2

5ï¸âƒ£ ç•°å¸¸å€¼æª¢æŸ¥
Sales: 23 å€‹ç•°å¸¸å€¼
Age: 5 å€‹ç•°å¸¸å€¼
```

### è¦–è¦ºåŒ–æª¢æŸ¥
```python
# è¦–è¦ºåŒ–å“è³ªå•é¡Œ
import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(2, 2, figsize=(12, 8))

# ç¼ºå¤±å€¼ç†±åŠ›åœ–
sns.heatmap(df.isnull(), cbar=True, ax=axes[0,0])
axes[0,0].set_title('ç¼ºå¤±å€¼åˆ†å¸ƒ')

# ç•°å¸¸å€¼ç®±å½¢åœ–
df.boxplot(column='Sales', ax=axes[0,1])
axes[0,1].set_title('éŠ·å”®é¡ç•°å¸¸å€¼')

# é‡è¤‡å€¼çµ±è¨ˆ
dup_counts = df.groupby(df.columns.tolist()).size()
axes[1,0].bar(['ç„¡é‡è¤‡', 'æœ‰é‡è¤‡'],
              [len(df) - duplicates, duplicates])
axes[1,0].set_title('é‡è¤‡å€¼çµ±è¨ˆ')

# è³‡æ–™åˆ†å¸ƒ
df['Sales'].hist(bins=30, ax=axes[1,1])
axes[1,1].set_title('éŠ·å”®é¡åˆ†å¸ƒ')

plt.tight_layout()
```

### å‹•ç•«æ•ˆæœ
- æª¢æŸ¥é …ç›®é€ä¸€å®Œæˆæ‰“å‹¾
- å•é¡Œå€åŸŸç´…è‰²è­¦ç¤º

### è¬›å¸«å£è¿°
```
"é€™å€‹æª¢æŸ¥æ¸…å–®æ˜¯ä½ çš„æ•‘å‘½å·¥å…·ã€‚
æ¯æ¬¡æ‹¿åˆ°è³‡æ–™ï¼Œå…ˆè·‘é€™å€‹å ±å‘Šï¼"
```

---

## ğŸ¬ P4ï¼šç¼ºå¤±å€¼è™•ç†ç­–ç•¥ [æ ¸å¿ƒæŠ€è¡“]
### ç‰ˆé¢é…ç½®
```
æ±ºç­–æ¨¹ + æ–¹æ³•æ¯”è¼ƒ
```

### å…§å®¹æ–‡å­—
**æ¨™é¡Œ** (32pt)
```
ğŸ”§ ç¼ºå¤±å€¼è™•ç†æ±ºç­–æ¨¹
```

**è™•ç†æ±ºç­–æµç¨‹**
```
ç¼ºå¤±å€¼æ¯”ä¾‹ï¼Ÿ
    â”œâ”€ > 60% â†’ è€ƒæ…®åˆªé™¤æ¬„ä½
    â”œâ”€ 30-60% â†’ è¬¹æ…è™•ç†
    â””â”€ < 30% â†’ å¯ä»¥å¡«è£œ
           â†“
        è³‡æ–™é¡å‹ï¼Ÿ
    â”œâ”€ æ•¸å€¼å‹ â†’ å¹³å‡/ä¸­ä½æ•¸/é æ¸¬
    â”œâ”€ é¡åˆ¥å‹ â†’ çœ¾æ•¸/æ–°é¡åˆ¥
    â””â”€ æ™‚é–“å‹ â†’ å‰å¾Œå¡«å……
```

### è™•ç†æ–¹æ³•æ¯”è¼ƒ
```python
# 1. åˆªé™¤æ³•
# é©ç”¨ï¼šç¼ºå¤±æ¯”ä¾‹å°ï¼Œéš¨æ©Ÿç¼ºå¤±
df_dropped = df.dropna()  # åˆªé™¤æœ‰ç¼ºå¤±çš„åˆ—
df_dropped = df.dropna(axis=1)  # åˆªé™¤æœ‰ç¼ºå¤±çš„æ¬„

# 2. å¡«è£œæ³• - ç°¡å–®å¡«è£œ
# æ•¸å€¼å‹ï¼šå¹³å‡å€¼/ä¸­ä½æ•¸
df['Age'].fillna(df['Age'].mean(), inplace=True)
df['Income'].fillna(df['Income'].median(), inplace=True)

# é¡åˆ¥å‹ï¼šçœ¾æ•¸æˆ–ç‰¹å®šå€¼
df['City'].fillna(df['City'].mode()[0], inplace=True)
df['Email'].fillna('unknown@email.com', inplace=True)

# 3. å¡«è£œæ³• - å‰å¾Œå¡«å……
# é©ç”¨ï¼šæ™‚é–“åºåˆ—è³‡æ–™
df['Date'].fillna(method='ffill', inplace=True)  # å‘å‰å¡«å……
df['Stock'].fillna(method='bfill', inplace=True)  # å‘å¾Œå¡«å……

# 4. å¡«è£œæ³• - æ’å€¼æ³•
# é©ç”¨ï¼šé€£çºŒæ•¸å€¼
df['Temperature'].interpolate(method='linear', inplace=True)

# 5. é€²éšï¼šé æ¸¬å¡«è£œ
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=5)
df_imputed = pd.DataFrame(
    imputer.fit_transform(df[numeric_cols]),
    columns=numeric_cols
)
```

### å¯¦æˆ°æ¡ˆä¾‹
```python
# å¯¦éš›è™•ç†ç­–ç•¥
def handle_missing(df):
    """æ™ºæ…§ç¼ºå¤±å€¼è™•ç†"""

    # è¨ˆç®—ç¼ºå¤±æ¯”ä¾‹
    missing_pct = df.isnull().sum() / len(df)

    # é«˜ç¼ºå¤±æ¬„ä½ï¼ˆ>60%ï¼‰- åˆªé™¤
    high_missing = missing_pct[missing_pct > 0.6].index
    df = df.drop(columns=high_missing)
    print(f"åˆªé™¤é«˜ç¼ºå¤±æ¬„ä½: {list(high_missing)}")

    # ä¸­åº¦ç¼ºå¤±ï¼ˆ30-60%ï¼‰- æ¨™è¨˜
    medium_missing = missing_pct[(missing_pct > 0.3) &
                                 (missing_pct <= 0.6)].index
    for col in medium_missing:
        df[f'{col}_was_missing'] = df[col].isnull().astype(int)

    # ä½ç¼ºå¤±ï¼ˆ<30%ï¼‰- å¡«è£œ
    for col in df.columns:
        if df[col].isnull().sum() > 0:
            if df[col].dtype in ['int64', 'float64']:
                df[col].fillna(df[col].median(), inplace=True)
            else:
                df[col].fillna('Unknown', inplace=True)

    return df

# æ‡‰ç”¨è™•ç†ç­–ç•¥
df_clean = handle_missing(df.copy())
print(f"è™•ç†å‰ç¼ºå¤±å€¼: {df.isnull().sum().sum()}")
print(f"è™•ç†å¾Œç¼ºå¤±å€¼: {df_clean.isnull().sum().sum()}")
```

### æ³¨æ„äº‹é …
```
âš ï¸ è™•ç†åŸå‰‡ï¼š
â€¢ äº†è§£ç¼ºå¤±åŸå› ï¼ˆéš¨æ©Ÿ/ç³»çµ±æ€§ï¼‰
â€¢ ä¿ç•™ç¼ºå¤±è¨Šæ¯ï¼ˆå¯èƒ½æœ‰æ„ç¾©ï¼‰
â€¢ é¿å…å¼•å…¥åå·®
â€¢ è¨˜éŒ„è™•ç†æ–¹æ³•
```

### å‹•ç•«æ•ˆæœ
- æ±ºç­–æ¨¹è·¯å¾‘å‹•ç•«
- å¡«è£œéç¨‹å¯è¦–åŒ–

### è¬›å¸«å£è¿°
```
"ç¼ºå¤±å€¼ä¸æ˜¯æ•µäººï¼Œè™•ç†ä¸ç•¶æ‰æ˜¯ã€‚
é¸å°æ–¹æ³•ï¼Œç¼ºå¤±å€¼ä¹Ÿèƒ½æä¾›è¨Šæ¯ï¼"
```

---

## ğŸ¬ P5ï¼šç•°å¸¸å€¼æª¢æ¸¬èˆ‡è™•ç† [é€²éšæŠ€è¡“]
### ç‰ˆé¢é…ç½®
```
æª¢æ¸¬æ–¹æ³• + è¦–è¦ºåŒ–
```

### å…§å®¹æ–‡å­—
**æ¨™é¡Œ** (32pt)
```
ğŸ¯ ç•°å¸¸å€¼ï¼šæ‰¾å‡ºè³‡æ–™ä¸­çš„æ€ªç¸
```

**æª¢æ¸¬æ–¹æ³•**
```python
# æ–¹æ³•1ï¼šIQR æ–¹æ³•ï¼ˆå››åˆ†ä½è·ï¼‰
def detect_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = df[(df[column] < lower_bound) |
                  (df[column] > upper_bound)]

    print(f"IQR æ–¹æ³•æª¢æ¸¬åˆ° {len(outliers)} å€‹ç•°å¸¸å€¼")
    print(f"ç¯„åœ: [{lower_bound:.2f}, {upper_bound:.2f}]")
    return outliers

# æ–¹æ³•2ï¼šZ-Scoreï¼ˆæ¨™æº–åˆ†æ•¸ï¼‰
def detect_outliers_zscore(df, column, threshold=3):
    z_scores = np.abs((df[column] - df[column].mean()) /
                      df[column].std())
    outliers = df[z_scores > threshold]

    print(f"Z-Score æ–¹æ³•æª¢æ¸¬åˆ° {len(outliers)} å€‹ç•°å¸¸å€¼")
    return outliers

# æ–¹æ³•3ï¼šIsolation Forestï¼ˆæ©Ÿå™¨å­¸ç¿’ï¼‰
from sklearn.ensemble import IsolationForest

def detect_outliers_ml(df, columns):
    iso_forest = IsolationForest(contamination=0.1,
                                 random_state=42)
    outliers = iso_forest.fit_predict(df[columns])

    df['is_outlier'] = outliers
    outlier_count = (outliers == -1).sum()
    print(f"Isolation Forest æª¢æ¸¬åˆ° {outlier_count} å€‹ç•°å¸¸å€¼")

    return df[outliers == -1]
```

### è¦–è¦ºåŒ–ç•°å¸¸å€¼
```python
# å¤šç¶­åº¦ç•°å¸¸å€¼è¦–è¦ºåŒ–
fig, axes = plt.subplots(2, 3, figsize=(15, 8))

# 1. ç®±å½¢åœ–
df.boxplot(column='Sales', ax=axes[0,0])
axes[0,0].set_title('ç®±å½¢åœ–æª¢æ¸¬')

# 2. æ•£é»åœ–
axes[0,1].scatter(df.index, df['Sales'])
axes[0,1].axhline(y=upper_bound, color='r', linestyle='--')
axes[0,1].axhline(y=lower_bound, color='r', linestyle='--')
axes[0,1].set_title('æ•£é»åœ–+ç•Œé™')

# 3. ç›´æ–¹åœ–
df['Sales'].hist(bins=30, ax=axes[0,2])
axes[0,2].axvline(x=upper_bound, color='r', linestyle='--')
axes[0,2].set_title('åˆ†å¸ƒåœ–')

# 4. Z-Score åˆ†å¸ƒ
z_scores = (df['Sales'] - df['Sales'].mean()) / df['Sales'].std()
z_scores.hist(bins=30, ax=axes[1,0])
axes[1,0].axvline(x=3, color='r', linestyle='--')
axes[1,0].axvline(x=-3, color='r', linestyle='--')
axes[1,0].set_title('Z-Score åˆ†å¸ƒ')

# 5. QQ Plot
from scipy import stats
stats.probplot(df['Sales'], dist="norm", plot=axes[1,1])
axes[1,1].set_title('Q-Q Plot')

# 6. æ™‚é–“åºåˆ—ç•°å¸¸
df['Date'] = pd.to_datetime(df['Date'])
axes[1,2].plot(df['Date'], df['Sales'], 'b-', alpha=0.5)
outlier_idx = detect_outliers_iqr(df, 'Sales').index
axes[1,2].scatter(df.loc[outlier_idx, 'Date'],
                  df.loc[outlier_idx, 'Sales'],
                  color='red', s=50)
axes[1,2].set_title('æ™‚é–“åºåˆ—ç•°å¸¸')

plt.tight_layout()
```

### è™•ç†ç­–ç•¥
```python
# ç•°å¸¸å€¼è™•ç†æ±ºç­–
def handle_outliers(df, column, method='cap'):
    """
    method: 'remove', 'cap', 'transform', 'keep'
    """

    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR

    if method == 'remove':
        # åˆªé™¤ç•°å¸¸å€¼
        df = df[(df[column] >= lower) & (df[column] <= upper)]

    elif method == 'cap':
        # æˆªå°¾è™•ç†
        df[column] = df[column].clip(lower=lower, upper=upper)

    elif method == 'transform':
        # è½‰æ›ï¼ˆå¦‚ logï¼‰
        df[column] = np.log1p(df[column])

    elif method == 'keep':
        # ä¿ç•™ä½†æ¨™è¨˜
        df[f'{column}_is_outlier'] = ((df[column] < lower) |
                                       (df[column] > upper))

    return df
```

### å•†æ¥­åˆ¤æ–·
```
ğŸ’¡ è™•ç†åŸå‰‡ï¼š
â€¢ ç•°å¸¸ â‰  éŒ¯èª¤ï¼ˆå¯èƒ½æ˜¯é‡è¦è¨Šæ¯ï¼‰
â€¢ äº†è§£æ¥­å‹™èƒŒæ™¯ï¼ˆVIPå®¢æˆ¶é«˜æ¶ˆè²»æ­£å¸¸ï¼‰
â€¢ åˆ†æç•°å¸¸åŸå› ï¼ˆä¿ƒéŠ·ã€ç¯€æ—¥ã€éŒ¯èª¤ï¼‰
â€¢ è¨˜éŒ„è™•ç†æ±ºç­–
```

### å‹•ç•«æ•ˆæœ
- ç•°å¸¸å€¼æ¨™è¨˜å‹•ç•«
- è™•ç†å‰å¾Œå°æ¯”

### è¬›å¸«å£è¿°
```
"ç•°å¸¸å€¼å¯èƒ½æ˜¯éŒ¯èª¤ï¼Œä¹Ÿå¯èƒ½æ˜¯æ©Ÿæœƒã€‚
é—œéµæ˜¯ç†è§£å®ƒï¼Œè€Œä¸æ˜¯ç›²ç›®åˆªé™¤ï¼"
```

---

## ğŸ¬ P6ï¼šé‡è¤‡å€¼è™•ç† [è³‡æ–™å”¯ä¸€æ€§]
### ç‰ˆé¢é…ç½®
```
é‡è¤‡é¡å‹ + è™•ç†æ–¹æ³•
```

### å…§å®¹æ–‡å­—
**æ¨™é¡Œ** (32pt)
```
ğŸ‘¥ é‡è¤‡å€¼ï¼šè³‡æ–™çš„é›™èƒèƒ
```

**é‡è¤‡å€¼é¡å‹**
```python
# 1. å®Œå…¨é‡è¤‡ - æ•´åˆ—ç›¸åŒ
full_duplicates = df[df.duplicated()]
print(f"å®Œå…¨é‡è¤‡: {len(full_duplicates)} åˆ—")

# 2. éƒ¨åˆ†é‡è¤‡ - é—œéµæ¬„ä½ç›¸åŒ
key_cols = ['Customer_ID', 'Date', 'Product']
partial_duplicates = df[df.duplicated(subset=key_cols)]
print(f"éƒ¨åˆ†é‡è¤‡: {len(partial_duplicates)} åˆ—")

# 3. æ¨¡ç³Šé‡è¤‡ - ç›¸ä¼¼ä½†ä¸å®Œå…¨ç›¸åŒ
# ä¾‹ï¼šç‹å°æ˜ vs ç‹ å°æ˜
from fuzzywuzzy import fuzz

def find_fuzzy_duplicates(df, column, threshold=90):
    """æ‰¾å‡ºç›¸ä¼¼çš„å€¼"""
    duplicates = []
    values = df[column].unique()

    for i, val1 in enumerate(values):
        for val2 in values[i+1:]:
            similarity = fuzz.ratio(str(val1), str(val2))
            if similarity > threshold:
                duplicates.append((val1, val2, similarity))

    return duplicates

fuzzy_dups = find_fuzzy_duplicates(df, 'Customer_Name')
print(f"ç›¸ä¼¼å§“å: {len(fuzzy_dups)} çµ„")
```

### è™•ç†ç­–ç•¥
```python
# é‡è¤‡å€¼è™•ç†æ–¹æ³•
def handle_duplicates(df, strategy='first'):
    """
    strategy: 'first', 'last', 'mean', 'max', 'sum'
    """

    print(f"è™•ç†å‰: {len(df)} åˆ—")

    if strategy in ['first', 'last']:
        # ä¿ç•™ç¬¬ä¸€å€‹æˆ–æœ€å¾Œä¸€å€‹
        df = df.drop_duplicates(keep=strategy)

    elif strategy == 'mean':
        # æ•¸å€¼æ¬„ä½å–å¹³å‡
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        agg_dict = {col: 'mean' for col in numeric_cols}

        # éæ•¸å€¼æ¬„ä½ä¿ç•™ç¬¬ä¸€å€‹
        non_numeric = df.select_dtypes(exclude=[np.number]).columns
        for col in non_numeric:
            agg_dict[col] = 'first'

        df = df.groupby('Customer_ID').agg(agg_dict).reset_index()

    elif strategy == 'sum':
        # é©ç”¨æ–¼äº¤æ˜“è³‡æ–™ - åˆä½µé‡‘é¡
        df = df.groupby(['Customer_ID', 'Date']).agg({
            'Sales': 'sum',
            'Quantity': 'sum',
            'Product': lambda x: ', '.join(x)
        }).reset_index()

    print(f"è™•ç†å¾Œ: {len(df)} åˆ—")
    return df
```

### å¯¦æˆ°æ¡ˆä¾‹
```python
# å®¢æˆ¶è³‡æ–™å»é‡
customer_df = df[['Customer_ID', 'Name', 'Email', 'Phone']]

# æª¢æŸ¥é‡è¤‡æƒ…æ³
print("é‡è¤‡æª¢æŸ¥å ±å‘Š:")
print("-" * 40)
print(f"Customer_ID é‡è¤‡: {customer_df['Customer_ID'].duplicated().sum()}")
print(f"Email é‡è¤‡: {customer_df['Email'].duplicated().sum()}")
print(f"Phone é‡è¤‡: {customer_df['Phone'].duplicated().sum()}")

# æ™ºæ…§å»é‡ç­–ç•¥
def smart_dedup(df):
    # 1. Customer_ID æ‡‰è©²å”¯ä¸€
    df = df.drop_duplicates(subset=['Customer_ID'], keep='last')

    # 2. Email æ¸…ç†ï¼ˆç§»é™¤ç„¡æ•ˆï¼‰
    df.loc[df['Email'].str.contains('test|temp|fake', na=False),
           'Email'] = None

    # 3. Phone æ¨™æº–åŒ–
    df['Phone'] = df['Phone'].str.replace('-', '').str.replace(' ', '')

    return df

customer_clean = smart_dedup(customer_df)
```

### é é˜²é‡è¤‡
```
âœ… é é˜²æªæ–½ï¼š
â€¢ è¨­å®šä¸»éµç´„æŸ
â€¢ å»ºç«‹å”¯ä¸€ç´¢å¼•
â€¢ è³‡æ–™è¼¸å…¥é©—è­‰
â€¢ å®šæœŸæª¢æŸ¥æ©Ÿåˆ¶
â€¢ ETL éç¨‹å»é‡
```

### å‹•ç•«æ•ˆæœ
- é‡è¤‡å€¼é«˜äº®é¡¯ç¤º
- åˆä½µéç¨‹å‹•ç•«

### è¬›å¸«å£è¿°
```
"é‡è¤‡å€¼åƒæ˜¯ç…§é¡å­ï¼Œ
è¦åˆ†æ¸…æ˜¯çœŸçš„é‡è¤‡é‚„æ˜¯ä¸åŒçš„è¨˜éŒ„ï¼"
```

---

## ğŸ¬ P7ï¼šè³‡æ–™å‹æ…‹è½‰æ› [æ ¼å¼æ¨™æº–åŒ–]
### ç‰ˆé¢é…ç½®
```
è½‰æ›éœ€æ±‚ + å¯¦ä½œæ–¹æ³•
```

### å…§å®¹æ–‡å­—
**æ¨™é¡Œ** (32pt)
```
ğŸ”„ è³‡æ–™å‹æ…‹ï¼šèªªå°çš„èªè¨€
```

**å¸¸è¦‹è½‰æ›éœ€æ±‚**
```python
# 1. å­—ä¸²è½‰æ•¸å€¼
# è™•ç†åƒåˆ†ä½ã€è²¨å¹£ç¬¦è™Ÿ
df['Price'] = df['Price'].str.replace('NT$', '')
df['Price'] = df['Price'].str.replace(',', '')
df['Price'] = pd.to_numeric(df['Price'], errors='coerce')

# 2. å­—ä¸²è½‰æ—¥æœŸ
# å„ç¨®æ—¥æœŸæ ¼å¼
df['Date1'] = pd.to_datetime(df['Date1'], format='%Y-%m-%d')
df['Date2'] = pd.to_datetime(df['Date2'], format='%Yå¹´%mæœˆ%dæ—¥')
df['Date3'] = pd.to_datetime(df['Date3'], format='%m/%d/%Y')

# æ™ºæ…§è§£æï¼ˆè¼ƒæ…¢ï¼‰
df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True)

# 3. æ•¸å€¼è½‰é¡åˆ¥
# åˆ†ç®±è™•ç†
df['Age_Group'] = pd.cut(df['Age'],
                         bins=[0, 18, 30, 50, 100],
                         labels=['å°‘å¹´', 'é’å¹´', 'ä¸­å¹´', 'è€å¹´'])

df['Income_Level'] = pd.qcut(df['Income'],
                             q=4,
                             labels=['ä½', 'ä¸­ä½', 'ä¸­é«˜', 'é«˜'])

# 4. é¡åˆ¥ç·¨ç¢¼
# Label Encoding
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['City_Code'] = le.fit_transform(df['City'])

# One-Hot Encoding
df_encoded = pd.get_dummies(df, columns=['Product_Type'],
                           prefix='Product')

# 5. å¸ƒæ—è½‰æ›
df['Is_VIP'] = df['Purchase_Amount'] > 10000
df['Has_Email'] = df['Email'].notna()
df['Is_Weekend'] = df['Date'].dt.dayofweek.isin([5, 6])
```

### å‹æ…‹å„ªåŒ–
```python
# è¨˜æ†¶é«”å„ªåŒ–
def optimize_dtypes(df):
    """å„ªåŒ–è³‡æ–™å‹æ…‹ä»¥ç¯€çœè¨˜æ†¶é«”"""

    start_mem = df.memory_usage().sum() / 1024**2
    print(f'åˆå§‹è¨˜æ†¶é«”: {start_mem:.2f} MB')

    # å„ªåŒ–æ•´æ•¸
    for col in df.select_dtypes(include=['int64']).columns:
        col_min = df[col].min()
        col_max = df[col].max()

        if col_min >= 0:  # ç„¡è² æ•¸
            if col_max < 255:
                df[col] = df[col].astype(np.uint8)
            elif col_max < 65535:
                df[col] = df[col].astype(np.uint16)
            elif col_max < 4294967295:
                df[col] = df[col].astype(np.uint32)
        else:
            if col_min > -128 and col_max < 127:
                df[col] = df[col].astype(np.int8)
            elif col_min > -32768 and col_max < 32767:
                df[col] = df[col].astype(np.int16)
            elif col_min > -2147483648 and col_max < 2147483647:
                df[col] = df[col].astype(np.int32)

    # å„ªåŒ–æµ®é»æ•¸
    for col in df.select_dtypes(include=['float64']).columns:
        df[col] = df[col].astype(np.float32)

    # å„ªåŒ–é¡åˆ¥
    for col in df.select_dtypes(include=['object']).columns:
        if df[col].nunique() / len(df) < 0.5:  # åŸºæ•¸è¼ƒä½
            df[col] = df[col].astype('category')

    end_mem = df.memory_usage().sum() / 1024**2
    print(f'å„ªåŒ–å¾Œè¨˜æ†¶é«”: {end_mem:.2f} MB')
    print(f'ç¯€çœ: {(1 - end_mem/start_mem)*100:.1f}%')

    return df

df_optimized = optimize_dtypes(df)
```

### å¯¦æˆ°ç¯„ä¾‹
```python
# å°ç£é›»è©±è™Ÿç¢¼æ¨™æº–åŒ–
def standardize_phone(phone):
    """æ¨™æº–åŒ–å°ç£é›»è©±è™Ÿç¢¼"""
    if pd.isna(phone):
        return None

    # ç§»é™¤æ‰€æœ‰éæ•¸å­—
    phone = ''.join(filter(str.isdigit, str(phone)))

    # æ‰‹æ©Ÿè™Ÿç¢¼
    if phone.startswith('09') and len(phone) == 10:
        return f'{phone[:4]}-{phone[4:7]}-{phone[7:]}'

    # å¸‚è©±ï¼ˆå«å€ç¢¼ï¼‰
    elif len(phone) >= 9:
        if phone.startswith('02'):  # å°åŒ—
            return f'{phone[:2]}-{phone[2:6]}-{phone[6:]}'
        elif phone.startswith('0'):  # å…¶ä»–åœ°å€
            return f'{phone[:3]}-{phone[3:6]}-{phone[6:]}'

    return phone  # ç„¡æ³•è­˜åˆ¥

df['Phone_Std'] = df['Phone'].apply(standardize_phone)
```

### å‹•ç•«æ•ˆæœ
- å‹æ…‹è½‰æ›æµç¨‹å‹•ç•«
- è¨˜æ†¶é«”ç¯€çœè¦–è¦ºåŒ–

### è¬›å¸«å£è¿°
```
"è³‡æ–™å‹æ…‹å°±åƒèªè¨€ï¼Œ
è½‰æ›æ­£ç¢ºæ‰èƒ½æ­£ç¢ºæºé€šï¼"
```

---

## ğŸ¬ P8ï¼šæ–‡å­—è³‡æ–™æ¸…ç† [å­—ä¸²è™•ç†]
### ç‰ˆé¢é…ç½®
```
æ¸…ç†æ–¹æ³• + å¯¦ä¾‹
```

### å…§å®¹æ–‡å­—
**æ¨™é¡Œ** (32pt)
```
ğŸ“ æ–‡å­—æ¸…ç†ï¼šè®“è³‡æ–™èªªäººè©±
```

**å¸¸è¦‹æ–‡å­—å•é¡Œ**
```python
# æ–‡å­—æ¸…ç†å®Œæ•´æµç¨‹
def clean_text(text):
    """æ¸…ç†æ–‡å­—è³‡æ–™"""
    if pd.isna(text):
        return text

    # 1. è½‰å­—ä¸²
    text = str(text)

    # 2. ç§»é™¤å¤šé¤˜ç©ºç™½
    text = text.strip()
    text = ' '.join(text.split())

    # 3. çµ±ä¸€å¤§å°å¯«
    # text = text.lower()  # è‹±æ–‡

    # 4. ç§»é™¤ç‰¹æ®Šå­—å…ƒ
    import re
    text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)

    # 5. ç¹ç°¡è½‰æ›
    # from opencc import OpenCC
    # cc = OpenCC('s2t')  # ç°¡é«”è½‰ç¹é«”
    # text = cc.convert(text)

    return text

# æ‰¹æ¬¡è™•ç†
df['Customer_Name'] = df['Customer_Name'].apply(clean_text)

# å¸¸è¦‹æ¸…ç†æ“ä½œ
# ç§»é™¤å‰å¾Œç©ºç™½
df['Product'] = df['Product'].str.strip()

# æ›¿æ›å­—å…ƒ
df['Address'] = df['Address'].str.replace('è‡º', 'å°')

# æå–è³‡è¨Š
df['Email_Domain'] = df['Email'].str.extract(r'@(.+)\.')[0]

# åˆ†å‰²æ¬„ä½
df[['First_Name', 'Last_Name']] = df['Full_Name'].str.split(' ',
                                                              expand=True)

# é•·åº¦æª¢æŸ¥
df['Name_Length'] = df['Customer_Name'].str.len()
```

### åœ°å€æ¨™æº–åŒ–
```python
# å°ç£åœ°å€è™•ç†
def parse_taiwan_address(address):
    """è§£æå°ç£åœ°å€"""
    if pd.isna(address):
        return pd.Series([None, None, None, None])

    import re

    # ç¸£å¸‚
    city_pattern = r'(å°åŒ—å¸‚|æ–°åŒ—å¸‚|æ¡ƒåœ’å¸‚|å°ä¸­å¸‚|å°å—å¸‚|é«˜é›„å¸‚|åŸºéš†å¸‚|æ–°ç«¹å¸‚|å˜‰ç¾©å¸‚|æ–°ç«¹ç¸£|è‹—æ —ç¸£|å½°åŒ–ç¸£|å—æŠ•ç¸£|é›²æ—ç¸£|å˜‰ç¾©ç¸£|å±æ±ç¸£|å®œè˜­ç¸£|èŠ±è“®ç¸£|å°æ±ç¸£|æ¾æ¹–ç¸£|é‡‘é–€ç¸£|é€£æ±Ÿç¸£)'
    city = re.search(city_pattern, address)
    city = city.group() if city else None

    # å€åŸŸ
    district_pattern = r'(\w{1,3}[å€é„‰é®å¸‚])'
    district = re.search(district_pattern, address)
    district = district.group() if district else None

    # è·¯è¡—
    road_pattern = r'(\w{1,10}[è·¯è¡—é“å··å¼„])'
    road = re.search(road_pattern, address)
    road = road.group() if road else None

    # è™Ÿ
    num_pattern = r'(\d+[è™Ÿä¹‹\-]\d*)'
    number = re.search(num_pattern, address)
    number = number.group() if number else None

    return pd.Series([city, district, road, number])

# æ‡‰ç”¨åœ°å€è§£æ
df[['City', 'District', 'Road', 'Number']] = df['Address'].apply(
    parse_taiwan_address
)
```

### å§“åè™•ç†
```python
# ä¸­æ–‡å§“åè™•ç†
def process_chinese_name(name):
    """è™•ç†ä¸­æ–‡å§“å"""
    if pd.isna(name) or len(name) < 2:
        return pd.Series([None, None])

    # å¸¸è¦‹è¤‡å§“
    compound_surnames = ['æ­é™½', 'å¸é¦¬', 'ä¸Šå®˜', 'è«¸è‘›']

    # åˆ¤æ–·å§“æ°
    if name[:2] in compound_surnames:
        surname = name[:2]
        given_name = name[2:]
    else:
        surname = name[0]
        given_name = name[1:]

    return pd.Series([surname, given_name])

df[['Surname', 'Given_Name']] = df['Customer_Name'].apply(
    process_chinese_name
)

# å§“åå»è­˜åˆ¥åŒ–
df['Name_Masked'] = df['Surname'] + 'O' * len(df['Given_Name'])
```

### å‹•ç•«æ•ˆæœ
- æ–‡å­—æ¸…ç†å‰å¾Œå°æ¯”
- è™•ç†æ­¥é©Ÿé€ä¸€å±•ç¤º

### è¬›å¸«å£è¿°
```
"æ–‡å­—è³‡æ–™æœ€é›£è™•ç†ï¼Œä½†ä¹Ÿæœ€æœ‰åƒ¹å€¼ã€‚
æ¸…ç†å¥½çš„æ–‡å­—èƒ½æŒ–å‡ºé‡‘ç¤¦ï¼"
```

---

## ğŸ¬ P9ï¼šæ—¥æœŸæ™‚é–“è™•ç† [æ™‚é–“åºåˆ—]
### ç‰ˆé¢é…ç½®
```
æ—¥æœŸæ“ä½œ + ç‰¹å¾µæå–
```

### å…§å®¹æ–‡å­—
**æ¨™é¡Œ** (32pt)
```
ğŸ“… æ™‚é–“è™•ç†ï¼šæŠ“ä½æ¯å€‹æ™‚åˆ»
```

**æ—¥æœŸæ™‚é–“æ“ä½œ**
```python
# 1. è§£ææ—¥æœŸ
df['Date'] = pd.to_datetime(df['Date'])

# 2. æå–æ™‚é–“ç‰¹å¾µ
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day
df['Weekday'] = df['Date'].dt.dayofweek  # 0=Monday
df['Weekday_Name'] = df['Date'].dt.day_name()
df['Quarter'] = df['Date'].dt.quarter
df['Week_of_Year'] = df['Date'].dt.isocalendar().week
df['Day_of_Year'] = df['Date'].dt.dayofyear
df['Hour'] = df['Date'].dt.hour

# 3. ç‰¹æ®Šæ™‚æœŸæ¨™è¨˜
df['Is_Weekend'] = df['Weekday'].isin([5, 6])
df['Is_Month_Start'] = df['Date'].dt.is_month_start
df['Is_Month_End'] = df['Date'].dt.is_month_end
df['Is_Quarter_End'] = df['Date'].dt.is_quarter_end

# 4. å°ç£ç¯€æ—¥æ¨™è¨˜
holidays_tw = {
    '2025-01-01': 'å…ƒæ—¦',
    '2025-01-29': 'é™¤å¤•',
    '2025-01-30': 'æ˜¥ç¯€',
    '2025-02-28': 'å’Œå¹³ç´€å¿µæ—¥',
    '2025-04-04': 'æ¸…æ˜ç¯€',
    '2025-05-01': 'å‹å‹•ç¯€',
    '2025-06-02': 'ç«¯åˆç¯€',
    '2025-09-08': 'ä¸­ç§‹ç¯€',
    '2025-10-10': 'åœ‹æ…¶æ—¥'
}

df['Is_Holiday'] = df['Date'].dt.strftime('%Y-%m-%d').isin(
    holidays_tw.keys()
)
df['Holiday_Name'] = df['Date'].dt.strftime('%Y-%m-%d').map(
    holidays_tw
)

# 5. æ™‚é–“å·®è¨ˆç®—
df['Days_Since'] = (pd.Timestamp.now() - df['Date']).dt.days
df['Time_Delta'] = df['Date'].diff()  # èˆ‡å‰ä¸€ç­†çš„æ™‚é–“å·®
```

### æ™‚æ®µåˆ†æ
```python
# ç‡Ÿæ¥­æ™‚æ®µåˆ†é¡
def classify_time_period(hour):
    """åˆ†é¡ç‡Ÿæ¥­æ™‚æ®µ"""
    if 6 <= hour < 11:
        return 'æ—©ä¸Š'
    elif 11 <= hour < 14:
        return 'ä¸­åˆ'
    elif 14 <= hour < 17:
        return 'ä¸‹åˆ'
    elif 17 <= hour < 21:
        return 'æ™šä¸Š'
    else:
        return 'æ·±å¤œ'

df['Period'] = df['Hour'].apply(classify_time_period)

# è³¼ç‰©å­£ç¯€
def get_shopping_season(month):
    """åˆ¤æ–·è³¼ç‰©å­£ç¯€"""
    if month in [1, 2]:
        return 'æ˜¥ç¯€æª”'
    elif month in [6, 7]:
        return 'å¹´ä¸­æ…¶'
    elif month in [9, 10]:
        return 'é€±å¹´æ…¶'
    elif month in [11, 12]:
        return 'é›™11/è–èª•'
    else:
        return 'ä¸€èˆ¬æ™‚æœŸ'

df['Shopping_Season'] = df['Month'].apply(get_shopping_season)
```

### æ™‚é–“åºåˆ—é‡æ¡æ¨£
```python
# è¨­å®šæ—¥æœŸç‚ºç´¢å¼•
df_ts = df.set_index('Date')

# æ¯æ—¥çµ±è¨ˆ
daily = df_ts['Sales'].resample('D').agg(['sum', 'mean', 'count'])

# æ¯é€±çµ±è¨ˆ
weekly = df_ts['Sales'].resample('W').sum()

# æ¯æœˆçµ±è¨ˆ
monthly = df_ts.resample('M').agg({
    'Sales': 'sum',
    'Customer_ID': 'nunique',
    'Invoice_ID': 'count'
})

# ç§»å‹•å¹³å‡
df_ts['MA7'] = df_ts['Sales'].rolling(window=7).mean()
df_ts['MA30'] = df_ts['Sales'].rolling(window=30).mean()
```

### è¦–è¦ºåŒ–æ™‚é–“æ¨¡å¼
```python
# æ™‚é–“æ¨¡å¼åˆ†æ
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# æ¯æ—¥éŠ·å”®è¶¨å‹¢
daily['sum'].plot(ax=axes[0,0])
axes[0,0].set_title('æ¯æ—¥éŠ·å”®è¶¨å‹¢')

# æ˜ŸæœŸæ¨¡å¼
weekday_sales = df.groupby('Weekday_Name')['Sales'].mean()
weekday_sales.plot(kind='bar', ax=axes[0,1])
axes[0,1].set_title('æ˜ŸæœŸéŠ·å”®æ¨¡å¼')

# å°æ™‚æ¨¡å¼
hour_sales = df.groupby('Hour')['Sales'].mean()
hour_sales.plot(ax=axes[1,0])
axes[1,0].set_title('24å°æ™‚éŠ·å”®æ¨¡å¼')

# æœˆä»½æ¨¡å¼
month_sales = df.groupby('Month')['Sales'].sum()
month_sales.plot(kind='bar', ax=axes[1,1])
axes[1,1].set_title('æœˆä»½éŠ·å”®æ¨¡å¼')

plt.tight_layout()
```

### å‹•ç•«æ•ˆæœ
- æ—¥æœŸè§£æéç¨‹
- æ™‚é–“è»¸å±•é–‹å‹•ç•«

### è¬›å¸«å£è¿°
```
"æ™‚é–“æ˜¯æœ€æœ‰åƒ¹å€¼çš„ç‰¹å¾µä¹‹ä¸€ã€‚
å¥½å¥½è™•ç†æ™‚é–“ï¼Œèƒ½çœ‹è¦‹åˆ¥äººçœ‹ä¸è¦‹çš„æ¨¡å¼ï¼"
```

---

## ğŸ¬ P10ï¼šç‰¹å¾µå·¥ç¨‹å…¥é–€ [åƒ¹å€¼å‰µé€ ]
### ç‰ˆé¢é…ç½®
```
ç‰¹å¾µå‰µé€ æ–¹æ³• + å¯¦ä¾‹
```

### å…§å®¹æ–‡å­—
**æ¨™é¡Œ** (32pt)
```
âœ¨ ç‰¹å¾µå·¥ç¨‹ï¼šé»çŸ³æˆé‡‘
```

**ç‰¹å¾µå·¥ç¨‹æ–¹æ³•**
```python
# 1. æ•¸å­¸é‹ç®—ç‰¹å¾µ
df['Total_Amount'] = df['Quantity'] * df['Unit_Price']
df['Discount_Rate'] = df['Discount'] / df['Original_Price']
df['Profit_Margin'] = (df['Sales'] - df['Cost']) / df['Sales']
df['Per_Unit_Revenue'] = df['Sales'] / df['Quantity']

# 2. çµ±è¨ˆç‰¹å¾µ
# å®¢æˆ¶å±¤ç´šçµ±è¨ˆ
customer_stats = df.groupby('Customer_ID').agg({
    'Sales': ['mean', 'sum', 'count', 'std'],
    'Date': ['min', 'max'],
    'Product_Category': 'nunique'
}).reset_index()

customer_stats.columns = ['Customer_ID', 'Avg_Purchase',
                          'Total_Purchase', 'Purchase_Count',
                          'Purchase_Std', 'First_Purchase',
                          'Last_Purchase', 'Category_Variety']

# åŠ å…¥ RFM ç‰¹å¾µ
today = pd.Timestamp.now()
customer_stats['Recency'] = (today - customer_stats['Last_Purchase']).dt.days
customer_stats['Frequency'] = customer_stats['Purchase_Count']
customer_stats['Monetary'] = customer_stats['Total_Purchase']

# 3. äº¤äº’ç‰¹å¾µ
df['Brand_Product'] = df['Brand'] + '_' + df['Product_Category']
df['Weekday_Hour'] = df['Weekday'].astype(str) + '_' + df['Hour'].astype(str)

# 4. æ¯”ç‡ç‰¹å¾µ
# ç›¸å°æŒ‡æ¨™
brand_avg = df.groupby('Brand')['Sales'].transform('mean')
df['Sales_vs_Brand_Avg'] = df['Sales'] / brand_avg

daily_avg = df.groupby(df['Date'].dt.date)['Sales'].transform('mean')
df['Sales_vs_Daily_Avg'] = df['Sales'] / daily_avg

# 5. ç´¯ç©ç‰¹å¾µ
df = df.sort_values('Date')
df['Cumsum_Sales'] = df.groupby('Customer_ID')['Sales'].cumsum()
df['Cumcount'] = df.groupby('Customer_ID').cumcount() + 1
```

### é€²éšç‰¹å¾µ
```python
# æ–‡å­—ç‰¹å¾µ
df['Product_Name_Length'] = df['Product_Name'].str.len()
df['Has_Discount_Keyword'] = df['Description'].str.contains(
    'æŠ˜æ‰£|ç‰¹åƒ¹|å„ªæƒ ', na=False
).astype(int)

# æ™‚é–“æ»¯å¾Œç‰¹å¾µ
df['Last_Purchase_Days'] = df.groupby('Customer_ID')['Date'].diff().dt.days
df['Sales_Lag1'] = df.groupby('Customer_ID')['Sales'].shift(1)
df['Sales_Change'] = df['Sales'] - df['Sales_Lag1']

# åˆ†ç¾¤ç‰¹å¾µ
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# å®¢æˆ¶åˆ†ç¾¤
features = ['Recency', 'Frequency', 'Monetary']
scaler = StandardScaler()
scaled_features = scaler.fit_transform(customer_stats[features])

kmeans = KMeans(n_clusters=4, random_state=42)
customer_stats['Cluster'] = kmeans.fit_predict(scaled_features)

# ç·¨ç¢¼é¡åˆ¥ç‰¹å¾µ
# Target Encodingï¼ˆè¬¹æ…ä½¿ç”¨ï¼Œé¿å…éæ“¬åˆï¼‰
target_mean = df.groupby('Product_Category')['Sales'].mean()
df['Category_Target_Encode'] = df['Product_Category'].map(target_mean)
```

### ç‰¹å¾µé‡è¦æ€§è©•ä¼°
```python
# è©•ä¼°ç‰¹å¾µé‡è¦æ€§
from sklearn.ensemble import RandomForestRegressor

# æº–å‚™è³‡æ–™
feature_cols = ['Quantity', 'Unit_Price', 'Discount_Rate',
                'Is_Weekend', 'Hour', 'Customer_Cluster']
X = df[feature_cols].fillna(0)
y = df['Sales']

# è¨“ç·´æ¨¡å‹
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X, y)

# ç‰¹å¾µé‡è¦æ€§
importance = pd.DataFrame({
    'Feature': feature_cols,
    'Importance': rf.feature_importances_
}).sort_values('Importance', ascending=False)

# è¦–è¦ºåŒ–
importance.plot(x='Feature', y='Importance', kind='barh')
plt.title('ç‰¹å¾µé‡è¦æ€§æ’å')
```

### å‹•ç•«æ•ˆæœ
- ç‰¹å¾µå‰µé€ éç¨‹å‹•ç•«
- é‡è¦æ€§æ’åå‹•æ…‹é¡¯ç¤º

### è¬›å¸«å£è¿°
```
"ç‰¹å¾µå·¥ç¨‹æ˜¯æ©Ÿå™¨å­¸ç¿’çš„éˆé­‚ã€‚
å¥½çš„ç‰¹å¾µå‹éè¤‡é›œçš„æ¨¡å‹ï¼"
```

---

## ğŸ¬ P11ï¼šè³‡æ–™æ¨™æº–åŒ–èˆ‡æ­£è¦åŒ– [å°ºåº¦çµ±ä¸€]
### ç‰ˆé¢é…ç½®
```
æ–¹æ³•æ¯”è¼ƒ + å¯¦ä½œ
```

### å…§å®¹æ–‡å­—
**æ¨™é¡Œ** (32pt)
```
ğŸ“ æ¨™æº–åŒ–ï¼šè®“è³‡æ–™ç«™åœ¨åŒä¸€èµ·è·‘ç·š
```

**ç‚ºä»€éº¼éœ€è¦æ¨™æº–åŒ–**
```
å•é¡Œï¼šä¸åŒå°ºåº¦çš„è³‡æ–™
å¹´é½¡ï¼š20-60ï¼ˆç¯„åœ 40ï¼‰
æ”¶å…¥ï¼š20000-1000000ï¼ˆç¯„åœ 980000ï¼‰

çµæœï¼šæ”¶å…¥æœƒä¸»å°æ‰€æœ‰åˆ†æï¼
```

### æ¨™æº–åŒ–æ–¹æ³•
```python
from sklearn.preprocessing import (StandardScaler, MinMaxScaler,
                                   RobustScaler, Normalizer)

# åŸå§‹è³‡æ–™
features = ['Age', 'Income', 'Purchase_Count']
X = df[features]

# 1. æ¨™æº–åŒ– (Z-Score Normalization)
# è½‰æ›ç‚ºå‡å€¼0ã€æ¨™æº–å·®1
scaler_standard = StandardScaler()
X_standard = scaler_standard.fit_transform(X)
df['Age_Std'] = X_standard[:, 0]
df['Income_Std'] = X_standard[:, 1]

# 2. æœ€å°æœ€å¤§æ­£è¦åŒ– (Min-Max Scaling)
# è½‰æ›åˆ° [0, 1] å€é–“
scaler_minmax = MinMaxScaler()
X_minmax = scaler_minmax.fit_transform(X)
df['Age_MinMax'] = X_minmax[:, 0]
df['Income_MinMax'] = X_minmax[:, 1]

# 3. å¼·å¥æ¨™æº–åŒ– (Robust Scaling)
# ä½¿ç”¨ä¸­ä½æ•¸å’Œ IQRï¼Œå°ç•°å¸¸å€¼ä¸æ•æ„Ÿ
scaler_robust = RobustScaler()
X_robust = scaler_robust.fit_transform(X)
df['Age_Robust'] = X_robust[:, 0]
df['Income_Robust'] = X_robust[:, 1]

# 4. æ­£è¦åŒ– (Normalization)
# è½‰æ›ç‚ºå–®ä½å‘é‡
normalizer = Normalizer()
X_normalized = normalizer.fit_transform(X)
```

### é¸æ“‡æŒ‡å—
```python
# ä½•æ™‚ä½¿ç”¨å“ªç¨®æ–¹æ³•
def choose_scaler(data_characteristics):
    """é¸æ“‡é©åˆçš„æ¨™æº–åŒ–æ–¹æ³•"""

    if data_characteristics == 'æœ‰ç•°å¸¸å€¼':
        return RobustScaler()
    elif data_characteristics == 'è³‡æ–™æœ‰ç•Œ':
        return MinMaxScaler(feature_range=(0, 1))
    elif data_characteristics == 'å¸¸æ…‹åˆ†å¸ƒ':
        return StandardScaler()
    elif data_characteristics == 'ç¨€ç–è³‡æ–™':
        return MaxAbsScaler()  # ä¿æŒç¨€ç–æ€§
    else:
        return StandardScaler()  # é è¨­

# è¦–è¦ºåŒ–æ¯”è¼ƒ
fig, axes = plt.subplots(2, 3, figsize=(15, 8))

# åŸå§‹åˆ†å¸ƒ
axes[0,0].hist(df['Income'], bins=30)
axes[0,0].set_title('åŸå§‹è³‡æ–™')

# StandardScaler
axes[0,1].hist(df['Income_Std'], bins=30)
axes[0,1].set_title('æ¨™æº–åŒ– (Z-Score)')

# MinMaxScaler
axes[0,2].hist(df['Income_MinMax'], bins=30)
axes[0,2].set_title('Min-Max æ­£è¦åŒ–')

# RobustScaler
axes[1,0].hist(df['Income_Robust'], bins=30)
axes[1,0].set_title('å¼·å¥æ¨™æº–åŒ–')

# æ¯”è¼ƒç®±å½¢åœ–
data_to_plot = [df['Income'], df['Income_Std'],
                df['Income_MinMax'], df['Income_Robust']]
axes[1,1].boxplot(data_to_plot)
axes[1,1].set_xticklabels(['åŸå§‹', 'Standard', 'MinMax', 'Robust'])
axes[1,1].set_title('æ¨™æº–åŒ–æ–¹æ³•æ¯”è¼ƒ')

plt.tight_layout()
```

### å¯¦æˆ°æ‡‰ç”¨
```python
# Pipeline æ•´åˆ
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

# å®šç¾©ä¸åŒæ¬„ä½çš„è™•ç†æ–¹å¼
numeric_features = ['Age', 'Income', 'Purchase_Count']
categorical_features = ['City', 'Product_Category']

# å»ºç«‹è½‰æ›å™¨
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(drop='first'), categorical_features)
    ])

# å»ºç«‹ Pipeline
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', RandomForestRegressor())
])

# è¨“ç·´æ¨¡å‹ï¼ˆè‡ªå‹•è™•ç†æ¨™æº–åŒ–ï¼‰
pipeline.fit(X_train, y_train)
```

### æ³¨æ„äº‹é …
```
âš ï¸ é‡è¦æé†’ï¼š
â€¢ è¨“ç·´è³‡æ–™ fit_transform()
â€¢ æ¸¬è©¦è³‡æ–™åª transform()
â€¢ ä¿å­˜ scaler ä¾›æœªä¾†ä½¿ç”¨
â€¢ è¨˜éŒ„è™•ç†åƒæ•¸
```

### å‹•ç•«æ•ˆæœ
- è³‡æ–™å°ºåº¦èª¿æ•´å‹•ç•«
- åˆ†å¸ƒè®ŠåŒ–éç¨‹

### è¬›å¸«å£è¿°
```
"æ¨™æº–åŒ–åƒæ˜¯è®“ä¸åŒå–®ä½çš„é¸æ‰‹å…¬å¹³ç«¶çˆ­ã€‚
é¸å°æ–¹æ³•ï¼Œè®“æ¨¡å‹è¡¨ç¾æ›´å¥½ï¼"
```

---

## ğŸ¬ P12ï¼šè³‡æ–™é©—è­‰èˆ‡å“è³ªä¿è­‰ [å“è³ªæ§åˆ¶]
### ç‰ˆé¢é…ç½®
```
é©—è­‰è¦å‰‡ + è‡ªå‹•æª¢æŸ¥
```

### å…§å®¹æ–‡å­—
**æ¨™é¡Œ** (32pt)
```
âœ… å“è³ªä¿è­‰ï¼šç¢ºä¿è³‡æ–™å¯ä¿¡
```

**è³‡æ–™é©—è­‰æ¡†æ¶**
```python
class DataValidator:
    """è³‡æ–™å“è³ªé©—è­‰å™¨"""

    def __init__(self, df):
        self.df = df
        self.errors = []
        self.warnings = []

    def validate_completeness(self, threshold=0.95):
        """å®Œæ•´æ€§é©—è­‰"""
        completeness = 1 - self.df.isnull().sum() / len(self.df)

        for col, rate in completeness.items():
            if rate < threshold:
                self.errors.append(
                    f"âŒ {col} å®Œæ•´ç‡ {rate:.1%} < {threshold:.0%}"
                )
        return self

    def validate_uniqueness(self, columns):
        """å”¯ä¸€æ€§é©—è­‰"""
        for col in columns:
            duplicates = self.df[col].duplicated().sum()
            if duplicates > 0:
                self.errors.append(
                    f"âŒ {col} æœ‰ {duplicates} å€‹é‡è¤‡å€¼"
                )
        return self

    def validate_range(self, rules):
        """ç¯„åœé©—è­‰"""
        for col, (min_val, max_val) in rules.items():
            out_of_range = self.df[
                (self.df[col] < min_val) | (self.df[col] > max_val)
            ]
            if len(out_of_range) > 0:
                self.warnings.append(
                    f"âš ï¸ {col} æœ‰ {len(out_of_range)} ç­†è¶…å‡ºç¯„åœ [{min_val}, {max_val}]"
                )
        return self

    def validate_consistency(self):
        """ä¸€è‡´æ€§é©—è­‰"""
        # é‚è¼¯æª¢æŸ¥
        invalid = self.df[self.df['Sales'] < self.df['Cost']]
        if len(invalid) > 0:
            self.errors.append(
                f"âŒ {len(invalid)} ç­†éŠ·å”®é¡å°æ–¼æˆæœ¬"
            )

        # æ—¥æœŸé‚è¼¯
        invalid_dates = self.df[self.df['End_Date'] < self.df['Start_Date']]
        if len(invalid_dates) > 0:
            self.errors.append(
                f"âŒ {len(invalid_dates)} ç­†çµæŸæ—¥æœŸæ—©æ–¼é–‹å§‹æ—¥æœŸ"
            )

        return self

    def get_report(self):
        """ç”¢ç”Ÿé©—è­‰å ±å‘Š"""
        print("=" * 50)
        print("ğŸ“‹ è³‡æ–™å“è³ªé©—è­‰å ±å‘Š")
        print("=" * 50)

        if not self.errors and not self.warnings:
            print("âœ… æ‰€æœ‰æª¢æŸ¥é€šéï¼")
        else:
            if self.errors:
                print("\nğŸš« éŒ¯èª¤ï¼š")
                for error in self.errors:
                    print(f"  {error}")

            if self.warnings:
                print("\nâš ï¸ è­¦å‘Šï¼š")
                for warning in self.warnings:
                    print(f"  {warning}")

        return len(self.errors) == 0

# ä½¿ç”¨é©—è­‰å™¨
validator = DataValidator(df)
is_valid = (validator
           .validate_completeness(0.9)
           .validate_uniqueness(['Invoice_ID'])
           .validate_range({'Age': (0, 120), 'Sales': (0, 1000000)})
           .validate_consistency()
           .get_report())

if not is_valid:
    print("\nâ— è³‡æ–™å“è³ªæœªé”æ¨™æº–ï¼Œéœ€è¦è™•ç†")
```

### è‡ªå‹•åŒ–å“è³ªç›£æ§
```python
# å“è³ªæŒ‡æ¨™è¿½è¹¤
def track_quality_metrics(df, save_path='quality_metrics.csv'):
    """è¿½è¹¤è³‡æ–™å“è³ªæŒ‡æ¨™"""

    metrics = {
        'Date': pd.Timestamp.now(),
        'Total_Rows': len(df),
        'Total_Columns': df.shape[1],
        'Missing_Rate': df.isnull().sum().sum() / (df.shape[0] * df.shape[1]),
        'Duplicate_Rate': df.duplicated().sum() / len(df),
        'Memory_MB': df.memory_usage().sum() / 1024**2,
        'Numeric_Cols': len(df.select_dtypes(include=[np.number]).columns),
        'Object_Cols': len(df.select_dtypes(include=['object']).columns)
    }

    # æ·»åŠ å„æ¬„ä½ç¼ºå¤±ç‡
    for col in df.columns:
        metrics[f'Missing_{col}'] = df[col].isnull().sum() / len(df)

    # å„²å­˜æˆ–æ›´æ–°è¨˜éŒ„
    if os.path.exists(save_path):
        history = pd.read_csv(save_path)
        history = pd.concat([history, pd.DataFrame([metrics])],
                           ignore_index=True)
    else:
        history = pd.DataFrame([metrics])

    history.to_csv(save_path, index=False)

    # è¶¨å‹¢åˆ†æ
    if len(history) > 1:
        print("ğŸ“ˆ å“è³ªè¶¨å‹¢ï¼š")
        print(f"ç¼ºå¤±ç‡è®ŠåŒ–: {history['Missing_Rate'].diff().iloc[-1]:+.2%}")
        print(f"è³‡æ–™é‡è®ŠåŒ–: {history['Total_Rows'].diff().iloc[-1]:+.0f} ç­†")

    return metrics
```

### å“è³ªå ±å‘Šç”Ÿæˆ
```python
def generate_quality_report(df, output_path='quality_report.html'):
    """ç”Ÿæˆ HTML å“è³ªå ±å‘Š"""

    html_content = f"""
    <html>
    <head>
        <title>è³‡æ–™å“è³ªå ±å‘Š</title>
        <style>
            body {{ font-family: Arial; margin: 20px; }}
            .metric {{
                display: inline-block;
                margin: 10px;
                padding: 15px;
                border: 1px solid #ddd;
                border-radius: 5px;
            }}
            .good {{ background-color: #d4edda; }}
            .warning {{ background-color: #fff3cd; }}
            .error {{ background-color: #f8d7da; }}
        </style>
    </head>
    <body>
        <h1>ğŸ“Š è³‡æ–™å“è³ªå ±å‘Š</h1>
        <p>ç”Ÿæˆæ™‚é–“: {pd.Timestamp.now()}</p>

        <h2>æ•´é«”æŒ‡æ¨™</h2>
        <div class="metric">è³‡æ–™ç­†æ•¸: {len(df):,}</div>
        <div class="metric">æ¬„ä½æ•¸é‡: {df.shape[1]}</div>
        <div class="metric">ç¼ºå¤±ç‡: {df.isnull().sum().sum()/(df.shape[0]*df.shape[1]):.2%}</div>

        <h2>å“è³ªè©•åˆ†</h2>
        <div class="metric {'good' if df.isnull().sum().sum() == 0 else 'warning'}">
            å®Œæ•´æ€§: {'âœ… å„ªç§€' if df.isnull().sum().sum() == 0 else 'âš ï¸ éœ€æ”¹å–„'}
        </div>

        <!-- æ›´å¤šå…§å®¹ -->
    </body>
    </html>
    """

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(html_content)

    print(f"âœ… å ±å‘Šå·²ç”Ÿæˆ: {output_path}")

generate_quality_report(df)
```

### å‹•ç•«æ•ˆæœ
- é©—è­‰é …ç›®é€ä¸€æª¢æŸ¥
- å“è³ªåˆ†æ•¸å³æ™‚æ›´æ–°

### è¬›å¸«å£è¿°
```
"å“è³ªä¿è­‰æ˜¯å°ˆæ¥­çš„è¡¨ç¾ã€‚
å»ºç«‹é©—è­‰æ©Ÿåˆ¶ï¼Œè®“éŒ¯èª¤ç„¡æ‰€éå½¢ï¼"
```

---

## ğŸ¬ P13ï¼šå¯¦æˆ°ç·´ç¿’ - æ¸…ç†é«’æ•¸æ“š [ç¶œåˆæ‡‰ç”¨]
### ç‰ˆé¢é…ç½®
```
ä»»å‹™èªªæ˜ + å®Œæ•´è§£æ±ºæ–¹æ¡ˆ
```

### å…§å®¹æ–‡å­—
**æ¨™é¡Œ** (32pt)
```
ğŸ† å¯¦æˆ°æŒ‘æˆ°ï¼šæ‹¯æ•‘é«’æ•¸æ“š
```

**ä»»å‹™èƒŒæ™¯**
```
ğŸ“‹ æƒ…å¢ƒï¼š
ä½ æ”¶åˆ°ä¸€ä»½é«’åˆ°ä¸è¡Œçš„å®¢æˆ¶è³‡æ–™
è€é—†è¦ä½ åœ¨ 30 åˆ†é˜å…§æ¸…ç†å®Œæˆ
é€™ä»½è³‡æ–™å°‡ç”¨æ–¼é‡è¦çš„å®¢æˆ¶åˆ†æ
```

**é«’æ•¸æ“šç¯„ä¾‹**
```python
# è¼‰å…¥é«’æ•¸æ“š
dirty_df = pd.DataFrame({
    'Customer_Name': ['ç‹å°æ˜', '  æå¤§è¯', 'å¼µç¾éº—', 'ç‹å°æ˜', None, 'é™³å»ºåœ‹ '],
    'Age': [25, 150, -5, 25, 30, 'thirty-five'],
    'Email': ['wang@gmail', 'lee@', 'zhang@yahoo.com', 'wang@gmail',
              'test@test', 'chen@gmail.com'],
    'Phone': ['0912-345-678', '0912345678', '091234567', '(09)12345678',
              None, '886912345678'],
    'Purchase_Date': ['2025-01-01', '2025/1/2', '01-03-2025', '2025-01-01',
                      '2025-13-01', '2025-01-32'],
    'Amount': ['1,000', '2000', 'NT$3000', '1,000', None, '500.5']
})

print("ğŸ˜± é«’æ•¸æ“šé è¦½ï¼š")
print(dirty_df)
```

### å®Œæ•´æ¸…ç†æ–¹æ¡ˆ
```python
def clean_dirty_data(df):
    """å®Œæ•´çš„é«’æ•¸æ“šæ¸…ç†æµç¨‹"""

    print("ğŸ§¹ é–‹å§‹æ¸…ç†è³‡æ–™...")
    df_clean = df.copy()

    # Step 1: è™•ç†å§“å
    print("Step 1: æ¸…ç†å§“å")
    df_clean['Customer_Name'] = df_clean['Customer_Name'].str.strip()
    df_clean['Customer_Name'].fillna('Unknown', inplace=True)

    # Step 2: è™•ç†å¹´é½¡
    print("Step 2: ä¿®æ­£å¹´é½¡")
    # è½‰æ›ç‚ºæ•¸å€¼
    df_clean['Age'] = pd.to_numeric(df_clean['Age'], errors='coerce')
    # è™•ç†ç•°å¸¸å€¼
    df_clean.loc[df_clean['Age'] > 120, 'Age'] = None
    df_clean.loc[df_clean['Age'] < 0, 'Age'] = None
    # å¡«è£œç¼ºå¤±
    df_clean['Age'].fillna(df_clean['Age'].median(), inplace=True)

    # Step 3: é©—è­‰ Email
    print("Step 3: é©—è­‰ Email")
    import re
    email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'

    def validate_email(email):
        if pd.isna(email):
            return None
        if re.match(email_pattern, str(email)):
            return email
        return None

    df_clean['Email'] = df_clean['Email'].apply(validate_email)

    # Step 4: æ¨™æº–åŒ–é›»è©±
    print("Step 4: æ¨™æº–åŒ–é›»è©±")
    def standardize_phone(phone):
        if pd.isna(phone):
            return None
        # ç§»é™¤æ‰€æœ‰éæ•¸å­—
        phone = ''.join(filter(str.isdigit, str(phone)))
        # è™•ç†åœ‹ç¢¼
        if phone.startswith('886'):
            phone = '0' + phone[3:]
        # æ ¼å¼åŒ–
        if len(phone) == 10 and phone.startswith('09'):
            return f'{phone[:4]}-{phone[4:7]}-{phone[7:]}'
        return None

    df_clean['Phone'] = df_clean['Phone'].apply(standardize_phone)

    # Step 5: è™•ç†æ—¥æœŸ
    print("Step 5: ä¿®æ­£æ—¥æœŸ")
    # å˜—è©¦å¤šç¨®æ ¼å¼
    date_formats = ['%Y-%m-%d', '%Y/%m/%d', '%m-%d-%Y']

    def parse_date(date_str):
        if pd.isna(date_str):
            return None
        for fmt in date_formats:
            try:
                return pd.to_datetime(date_str, format=fmt)
            except:
                continue
        return None

    df_clean['Purchase_Date'] = df_clean['Purchase_Date'].apply(parse_date)

    # Step 6: æ¸…ç†é‡‘é¡
    print("Step 6: è™•ç†é‡‘é¡")
    df_clean['Amount'] = (df_clean['Amount']
                          .str.replace('NT$', '')
                          .str.replace(',', '')
                          .str.replace('$', ''))
    df_clean['Amount'] = pd.to_numeric(df_clean['Amount'], errors='coerce')
    df_clean['Amount'].fillna(0, inplace=True)

    # Step 7: å»é™¤é‡è¤‡
    print("Step 7: å»é™¤é‡è¤‡")
    df_clean = df_clean.drop_duplicates(subset=['Customer_Name', 'Email'],
                                        keep='first')

    print("âœ… æ¸…ç†å®Œæˆï¼")
    return df_clean

# åŸ·è¡Œæ¸…ç†
clean_df = clean_dirty_data(dirty_df)

print("\nğŸ“Š æ¸…ç†çµæœï¼š")
print(clean_df)

# å“è³ªæª¢æŸ¥
print("\nğŸ“ˆ å“è³ªæ”¹å–„ï¼š")
print(f"ç¼ºå¤±å€¼: {dirty_df.isnull().sum().sum()} â†’ {clean_df.isnull().sum().sum()}")
print(f"é‡è¤‡å€¼: {dirty_df.duplicated().sum()} â†’ {clean_df.duplicated().sum()}")
print(f"æœ‰æ•ˆ Email: {dirty_df['Email'].notna().sum()} â†’ {clean_df['Email'].notna().sum()}")
```

### æ¸…ç†å‰å¾Œå°æ¯”
```python
# è¦–è¦ºåŒ–æ”¹å–„æ•ˆæœ
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# æ¸…ç†å‰
missing_before = dirty_df.isnull().sum()
ax1.bar(missing_before.index, missing_before.values, color='red')
ax1.set_title('æ¸…ç†å‰ç¼ºå¤±å€¼')
ax1.set_xlabel('æ¬„ä½')
ax1.set_ylabel('ç¼ºå¤±æ•¸é‡')
ax1.tick_params(axis='x', rotation=45)

# æ¸…ç†å¾Œ
missing_after = clean_df.isnull().sum()
ax2.bar(missing_after.index, missing_after.values, color='green')
ax2.set_title('æ¸…ç†å¾Œç¼ºå¤±å€¼')
ax2.set_xlabel('æ¬„ä½')
ax2.set_ylabel('ç¼ºå¤±æ•¸é‡')
ax2.tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()
```

### è¬›å¸«å£è¿°
```
"é€™å°±æ˜¯çœŸå¯¦ä¸–ç•Œçš„è³‡æ–™ï¼
å­¸æœƒæ¸…ç†é«’æ•¸æ“šï¼Œä½ å°±æ˜¯è³‡æ–™çš„æ•‘ä¸–ä¸»ï¼"
```

---

## ğŸ¬ P14ï¼šæ¸…ç†æµç¨‹è‡ªå‹•åŒ– [æ•ˆç‡æå‡]
### ç‰ˆé¢é…ç½®
```
è‡ªå‹•åŒ–æ¡†æ¶ + å¯¦ä½œ
```

### å…§å®¹æ–‡å­—
**æ¨™é¡Œ** (32pt)
```
ğŸ¤– è‡ªå‹•åŒ–ï¼šä¸€éµæ¸…ç†è³‡æ–™
```

**è‡ªå‹•åŒ–æ¸…ç†æ¡†æ¶**
```python
class AutoDataCleaner:
    """è‡ªå‹•åŒ–è³‡æ–™æ¸…ç†å™¨"""

    def __init__(self, config=None):
        self.config = config or self.default_config()
        self.cleaning_log = []

    def default_config(self):
        """é è¨­æ¸…ç†é…ç½®"""
        return {
            'missing_threshold': 0.6,  # ç¼ºå¤±å€¼åˆªé™¤é–¾å€¼
            'duplicate_cols': None,     # å»é‡æ¬„ä½
            'date_cols': [],           # æ—¥æœŸæ¬„ä½
            'numeric_cols': [],        # æ•¸å€¼æ¬„ä½
            'categorical_cols': [],    # é¡åˆ¥æ¬„ä½
            'outlier_method': 'iqr',   # ç•°å¸¸å€¼æ–¹æ³•
            'fill_strategy': 'smart'   # å¡«è£œç­–ç•¥
        }

    def auto_detect_types(self, df):
        """è‡ªå‹•æª¢æ¸¬æ¬„ä½é¡å‹"""
        for col in df.columns:
            # å˜—è©¦è½‰æ›ç‚ºæ—¥æœŸ
            try:
                pd.to_datetime(df[col], infer_datetime_format=True)
                self.config['date_cols'].append(col)
                continue
            except:
                pass

            # å˜—è©¦è½‰æ›ç‚ºæ•¸å€¼
            try:
                pd.to_numeric(df[col])
                self.config['numeric_cols'].append(col)
                continue
            except:
                pass

            # å…¶é¤˜ç‚ºé¡åˆ¥
            self.config['categorical_cols'].append(col)

        self.log("è‡ªå‹•æª¢æ¸¬æ¬„ä½é¡å‹å®Œæˆ")
        return self

    def clean(self, df):
        """åŸ·è¡Œå®Œæ•´æ¸…ç†æµç¨‹"""
        df_clean = df.copy()

        # 1. ç§»é™¤é«˜ç¼ºå¤±æ¬„ä½
        df_clean = self.remove_high_missing(df_clean)

        # 2. è™•ç†é‡è¤‡å€¼
        df_clean = self.remove_duplicates(df_clean)

        # 3. è™•ç†ç¼ºå¤±å€¼
        df_clean = self.handle_missing(df_clean)

        # 4. è™•ç†ç•°å¸¸å€¼
        df_clean = self.handle_outliers(df_clean)

        # 5. æ¨™æº–åŒ–æ ¼å¼
        df_clean = self.standardize_formats(df_clean)

        # 6. é©—è­‰çµæœ
        self.validate_results(df, df_clean)

        return df_clean

    def remove_high_missing(self, df):
        """ç§»é™¤é«˜ç¼ºå¤±æ¬„ä½"""
        missing_rate = df.isnull().sum() / len(df)
        high_missing = missing_rate[missing_rate > self.config['missing_threshold']]

        if len(high_missing) > 0:
            df = df.drop(columns=high_missing.index)
            self.log(f"ç§»é™¤é«˜ç¼ºå¤±æ¬„ä½: {list(high_missing.index)}")

        return df

    def remove_duplicates(self, df):
        """å»é™¤é‡è¤‡å€¼"""
        before = len(df)

        if self.config['duplicate_cols']:
            df = df.drop_duplicates(subset=self.config['duplicate_cols'])
        else:
            df = df.drop_duplicates()

        after = len(df)
        if before > after:
            self.log(f"ç§»é™¤ {before - after} ç­†é‡è¤‡è³‡æ–™")

        return df

    def handle_missing(self, df):
        """æ™ºæ…§å¡«è£œç¼ºå¤±å€¼"""
        for col in df.columns:
            if df[col].isnull().sum() == 0:
                continue

            if col in self.config['numeric_cols']:
                # æ•¸å€¼å‹ï¼šä¸­ä½æ•¸
                df[col].fillna(df[col].median(), inplace=True)
            elif col in self.config['categorical_cols']:
                # é¡åˆ¥å‹ï¼šçœ¾æ•¸æˆ– 'Unknown'
                mode = df[col].mode()
                if len(mode) > 0:
                    df[col].fillna(mode[0], inplace=True)
                else:
                    df[col].fillna('Unknown', inplace=True)
            elif col in self.config['date_cols']:
                # æ—¥æœŸå‹ï¼šå‰å‘å¡«å……
                df[col].fillna(method='ffill', inplace=True)

        self.log("ç¼ºå¤±å€¼è™•ç†å®Œæˆ")
        return df

    def handle_outliers(self, df):
        """è™•ç†ç•°å¸¸å€¼"""
        if self.config['outlier_method'] == 'iqr':
            for col in self.config['numeric_cols']:
                if col in df.columns:
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    lower = Q1 - 1.5 * IQR
                    upper = Q3 + 1.5 * IQR

                    # æˆªå°¾è™•ç†
                    before = df[col].clip(lower=lower, upper=upper)
                    outliers = ((df[col] < lower) | (df[col] > upper)).sum()

                    if outliers > 0:
                        df[col] = df[col].clip(lower=lower, upper=upper)
                        self.log(f"{col}: è™•ç† {outliers} å€‹ç•°å¸¸å€¼")

        return df

    def standardize_formats(self, df):
        """æ¨™æº–åŒ–æ ¼å¼"""
        # æ–‡å­—æ¬„ä½å»ç©ºç™½
        for col in self.config['categorical_cols']:
            if col in df.columns and df[col].dtype == 'object':
                df[col] = df[col].str.strip()

        # æ—¥æœŸæ¬„ä½è½‰æ›
        for col in self.config['date_cols']:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')

        self.log("æ ¼å¼æ¨™æº–åŒ–å®Œæˆ")
        return df

    def validate_results(self, df_before, df_after):
        """é©—è­‰æ¸…ç†çµæœ"""
        print("\n" + "=" * 50)
        print("ğŸ“Š æ¸…ç†å ±å‘Š")
        print("=" * 50)

        print(f"è³‡æ–™ç­†æ•¸: {len(df_before)} â†’ {len(df_after)}")
        print(f"æ¬„ä½æ•¸é‡: {df_before.shape[1]} â†’ {df_after.shape[1]}")
        print(f"ç¼ºå¤±å€¼: {df_before.isnull().sum().sum()} â†’ {df_after.isnull().sum().sum()}")

        print("\nğŸ“ æ¸…ç†æ—¥èªŒ:")
        for log in self.cleaning_log:
            print(f"  â€¢ {log}")

    def log(self, message):
        """è¨˜éŒ„æ¸…ç†æ­¥é©Ÿ"""
        self.cleaning_log.append(message)

# ä½¿ç”¨è‡ªå‹•æ¸…ç†å™¨
cleaner = AutoDataCleaner()
cleaner.auto_detect_types(dirty_df)
clean_df = cleaner.clean(dirty_df)
```

### å‹•ç•«æ•ˆæœ
- è‡ªå‹•åŒ–æµç¨‹å‹•ç•«
- æ¸…ç†æ­¥é©Ÿé€ä¸€å®Œæˆ

### è¬›å¸«å£è¿°
```
"è‡ªå‹•åŒ–è®“ä½ å¾é‡è¤‡å‹å‹•ä¸­è§£æ”¾ã€‚
å¯«ä¸€æ¬¡ï¼Œç”¨ä¸€è¼©å­ï¼"
```

---

## ğŸ¬ P15ï¼šæ¨¡çµ„ç¸½çµ [æˆå°±ç¢ºèª]
### ç‰ˆé¢é…ç½®
```
æŠ€èƒ½æ¸…å–® + æˆå°±ç³»çµ±
```

### å…§å®¹æ–‡å­—
**æ¨™é¡Œ** (32pt)
```
ğŸ‰ Module 3 å®Œæˆï¼
```

**æˆå°±è§£é–**
```
ğŸ† æˆå°±è§£é–ï¼š
ã€Œè³‡æ–™æ¸…ç†å°ˆå®¶ã€
- æŒæ¡å®Œæ•´æ¸…ç†æµç¨‹
- è™•ç†å„é¡é«’æ•¸æ“š
- å»ºç«‹å“è³ªä¿è­‰ç³»çµ±
```

**æŠ€èƒ½ç¸½çµ**
```
âœ… ä½ ç¾åœ¨æŒæ¡çš„æŠ€èƒ½ï¼š
â–¡ ç³»çµ±åŒ–å“è³ªæª¢æŸ¥
â–¡ ç¼ºå¤±å€¼è™•ç†ç­–ç•¥
â–¡ ç•°å¸¸å€¼æª¢æ¸¬æ–¹æ³•
â–¡ é‡è¤‡å€¼è­˜åˆ¥å»é™¤
â–¡ è³‡æ–™å‹æ…‹è½‰æ›
â–¡ æ–‡å­—è³‡æ–™æ¸…ç†
â–¡ æ—¥æœŸæ™‚é–“è™•ç†
â–¡ ç‰¹å¾µå·¥ç¨‹åŸºç¤
â–¡ è³‡æ–™æ¨™æº–åŒ–
â–¡ è‡ªå‹•åŒ–æ¸…ç†æµç¨‹

ğŸ’ª ä½ èƒ½è§£æ±ºçš„å•é¡Œï¼š
â€¢ é«’æ•¸æ“šè®Šé»ƒé‡‘
â€¢ æå‡è³‡æ–™å“è³ª 90%
â€¢ ç¯€çœæ¸…ç†æ™‚é–“ 80%
â€¢ ç¢ºä¿åˆ†æå¯ä¿¡åº¦
```

### å¯¦æˆ°æˆæœ
```
ğŸ“Š æœ¬æ¨¡çµ„æˆæœï¼š
â€¢ æ¸…ç†é«’æ•¸æ“šï¼š100%
â€¢ å“è³ªæå‡ï¼š85%
â€¢ è‡ªå‹•åŒ–ç¨‹åº¦ï¼š70%
â€¢ æ™‚é–“ç¯€çœï¼š2å°æ™‚ â†’ 5åˆ†é˜
```

### ä¸‹éšæ®µé å‘Š
```
â­ï¸ Module 4 é å‘Šï¼š
ã€Œæ•¸æ“šè¦–è¦ºåŒ–èˆ‡åœ–è¡¨é¸ç”¨ã€
â€¢ åœ–è¡¨é¸æ“‡æ±ºç­–æ¨¹
â€¢ Matplotlib åŸºç¤
â€¢ Seaborn çµ±è¨ˆåœ–
â€¢ äº’å‹•å¼å„€è¡¨æ¿
```

### å‹•ç•«æ•ˆæœ
- æˆå°±å¾½ç« æ—‹è½‰
- æŠ€èƒ½é»äº®æ•ˆæœ

### è¬›å¸«å£è¿°
```
"æ­å–œï¼ä½ å·²ç¶“æ˜¯è³‡æ–™æ¸…ç†å°ˆå®¶äº†ï¼
ä¹¾æ·¨çš„è³‡æ–™æ˜¯å¥½åˆ†æçš„é–‹å§‹ã€‚
æ¥ä¸‹ä¾†ï¼Œè®“è³‡æ–™èªªæ•…äº‹ï¼"
```

---

## ğŸ“‹ Module 3 æ•™å­¸æª¢æ ¸è¡¨

### æ™‚é–“æ§åˆ¶ (45åˆ†é˜)
- [ ] P1-P3: 8åˆ†é˜ (å•é¡Œæ„è­˜)
- [ ] P4-P7: 15åˆ†é˜ (æ ¸å¿ƒæŠ€è¡“)
- [ ] P8-P11: 12åˆ†é˜ (é€²éšè™•ç†)
- [ ] P12-P15: 10åˆ†é˜ (å¯¦æˆ°æ‡‰ç”¨)

### é—œéµæª¢æŸ¥é»
- [ ] P3: å“è³ªæª¢æŸ¥å ±å‘ŠæˆåŠŸ
- [ ] P4: ç¼ºå¤±å€¼è™•ç†ç†è§£
- [ ] P5: ç•°å¸¸å€¼æª¢æ¸¬æŒæ¡
- [ ] P13: å®Œæˆé«’æ•¸æ“šæ¸…ç†

### æ•™ææº–å‚™
- [ ] é«’æ•¸æ“šç¯„ä¾‹æª”æ¡ˆ
- [ ] æ¸…ç†ç¨‹å¼ç¢¼æ¨¡æ¿
- [ ] å“è³ªå ±å‘Šç¯„æœ¬
- [ ] è‡ªå‹•åŒ–æ¡†æ¶

---

*Module 3 è¨­è¨ˆå®Œæˆ - ç¢ºä¿å­¸å“¡æŒæ¡è³‡æ–™å“è³ªè™•ç†æ ¸å¿ƒæŠ€èƒ½*